AWS Certified Cloud PRactitioner Exam Prep Transcripts

Welcome and course introduction

This module has one video.


Welcome
–
Hi everyone. I'm Julie Elkins, a senior curriculum developer for the AWS Exam Prep team. Our team builds content to help learners prepare for each of the AWS Certifications. 

Welcome to the exam prep course for the AWS Certified Cloud Practitioner. This course is intended for learners who meet three requirements. First, you should have up to 6 months of exposure to AWS Cloud design, implementation, and operations. Second, you should come from a non-IT background. Third, you are preparing for the AWS Certified Cloud Practitioner exam, version C02. This course is our basic or free version. 

If you want additional exam-style questions, hands-on labs, flash cards, a practice exam, and more, subscribe to the AWS Skill Builder for the enhanced version. There is a link in the course description. 

I want to provide an overview of our step-by-step process for preparing for the AWS certifications, and how they relate to this course. 

Step 1 is to get to know the exam. In Module 1, you will have the chance to download and review the exam guide. 

Step 2 is to get to know the exam style questions. In Module 2, you will learn what exam style questions are, and how to familiarize yourself with them. You'll also have a chance to take the official question set, so that you can understand the level and depth of the questions for this exam. 

Step 3 is to learn about exam topics. Hopefully, you already have the required knowledge to take this exam. If not, Module 3 provides recommended training to help you learn the exam topics. While there is no exact training required to take an AWS certification, we offer suggestions for digital courses, builder labs, AWS Cloud Quest, and AWS Jam Journeys. Note that a digital subscription is required for all labs, most Cloud Quest roles, and all Jam Journeys. 

Step 4 is to review everything that is tested on the exam. This is the main part of the course. Module 4 is organized by exam domain. Each exam domain includes video lessons that follow the task statements for each domain. At the end of each domain are lessons that cover sample exam style questions. Here I'll provide tips on how to read and answer these questions. At the end of the domain, there is a list of additional resources for more information on the content covered. Note that there are additional resources available in the enhanced version of this course. 

After the video lessons, each domain includes practice exam style questions that you take on your own. These are called bonus questions. You can also test your knowledge of key concepts with flash cards. You can also gain additional practice with hands-on labs at the ends of Domain 2 and 3. 

As we cover each domain, you should evaluate whether you are ready to move forward to take the exam, or if you need more preparation in depth into the topics in each domain. 

This course does not include every service in detail that you'll need to pass the exam. This course is intended to help you determine whether you are ready to take the exam, and to help you discover gaps in your knowledge so that you can plan for further study. 

Step 5 is assessing your exam readiness. With the enhanced version of this course, you gain access to the official practice exam. Use the practice exam to determine whether you're ready to take your exam. 

Step 6 is registering for the exam. The end of this course contains resources about registering for the exam. If you plan to take your exam online, definitely watch "what to expect taking an online proctored exam." Finally, join us for the course close. 

Now let's move on to step 1, getting to know the exam. In this foundational course from Amazon Web Services, you will learn how to assess your preparedness for the AWS Certified Cloud Practitioner exam. 

This exam validates that you have the technical expertise to explain the value of AWS, and to understand the AWS Shared Responsibility Model, security best practices, AWS cost, economics, and billing practices. 

This exam also validates that you understand AWS core services, and identify AWS services for different use cases. 

Throughout this course, you will do the following. Investigate each domain, and learn what you should know to demonstrate overall knowledge of AWS, and review sample walkthrough exam questions from the exam domains, and learn how to interpret the concepts being tested, so that you can better eliminate incorrect responses. 

Now, when exploring the requirements of the certification, one of the first resources you should consult is the AWS Certified Cloud Practitioner exam guide. The exam guide outlines the topics covered in the certification exam. You can find a link to the exam guide in Module 1. 

Domain 1 is Cloud Concepts, and it covers 24% of the exam. This domain focuses on knowing and understanding the benefits of AWS, design principles, migration to AWS, and cost optimization using AWS. 

Domain 2 is Security and Compliance, and it covers 30% of the exam. This domain focuses on the AWS Shared Responsibility Model, understanding security, governance, and compliance concepts, the importance of access management, and how to identify components and resources for security. 

Domain 3 is Cloud Technology and Services, and it covers 34% of the exam. For this domain, the focus is on understanding methods to deploy and operate in AWS, the AWS global infrastructure, and AWS services for compute, database, network, storage, machine learning, analytic, application, developer, end-user, and more. 

Domain 4 is Billing, Pricing, and Support, and it covers 12% of the exam. For this domain, ensure you can compare AWS pricing models, resources for cost management, AWS support options, and technical resources. 

Good luck completing this course. Let's get started with Domain 1.

Domain 1: Cloud Concepts

This module has seven videos.


Domain 1 Introduction
–
Let's get started with Domain 1 covering cloud concepts such as what is AWS, the benefits of AWS, design principles, migration, and economics. 

Domain 1: Cloud concepts is broken into four task statements that we will discuss over the next few lessons. 

Task statement 1.1: Define the benefits of the AWS Cloud. 
Task statement 1.2: Identify design principles of the AWS Cloud. 
Task statement 1.3: Understand the benefits of and strategies for migration to the AWS Cloud.
And task statement 1.4: Understand concepts of cloud economics. 
For the first task statement, we will start with AWS fundamentals. Cloud computing isn't a buzzword. It is a unique type of computing that has a formal set of definitions. We will dive deeper into what is AWS and step through what makes cloud, cloud. 



For the second task statement, we will focus on design principles for AWS and talk about the AWS Well-Architected Framework, best practices, and core strategies for our architecting systems in the cloud. 

For the third task statement, we will focus on reducing database costs and improving availability when you migrate to AWS. We will talk about the AWS storage services and ensure you understand the type of data that you have and what data store is needed. 

For the fourth task statement, we will focus on understanding the concepts of cloud economics and multiple ways to run a cost-optimized environment. 

Over the next several videos in this module, I will address each task statement individually, breaking down the knowledge and skills expected of you to be successful. 

Let's get started to evaluate your readiness for the exam in the next video where we will cover the first task statement from Domain 1, define the benefits of the AWS Cloud.


Define the benefits of the AWS Cloud
–
Let's get started with the first task statement, which is to define the benefits of the AWS Cloud. For this task statement, ensure you know the value of AWS. 

But before you can understand the value of AWS, ensure you know what AWS is. Can you define AWS? 

Well, AWS says that Amazon Web Service is the world's most comprehensive and broadly adopted cloud, offering over 200 fully featured services from data centers globally. Millions of customers, including the fastest growing startups, largest enterprises, and leading government agencies are using AWS to lower costs, become more agile, and innovate faster. 

So AWS provides us a way to power our infrastructure, while becoming more agile and also lowering our costs. But you also need to understand what cloud computing is. And if you ask around or you Google what is cloud computing, you will find multiple different answers. But cloud computing basically means that you have to meet certain criteria. 

Let's break it down and quickly walk through the five criteria. 

The first criteria we have to meet for AWS to be considered cloud computing is that AWS has to offer an on-demand self-service, which is the ability to provision compute features or capabilities like servers, databases, network, storage, and more on demand when that service is needed. Also, cloud computing has to be able to provision resources without any human intervention or interaction to make it an on-demand self-service. AWS gives us the ability to do this from the console, the command line, and also by using application programming interfaces. We do not need to request or ask anyone. With the correct permissions, you can go in and start building. 

The second criteria is that cloud computing needs to have network connectivity to access and provision resources that are needed. AWS gives us the ability to build using the AWS Management Console, the command line, HTTP, HTTPS, VPN, SSH, and more. 

The third criteria is that cloud computing needs resource pooling. Well, what is resource pooling? AWS provides pooled resources to serve multiple different AWS customers. AWS has thousands of servers, databases, and more that we can on-demand provision and build in our own environment. And these pooled resources are available to me, to you, and to others as well. But another part of this third criteria that we have to meet is that we do not know the exact location of these pooled resources. 

For example, when you spin up an Amazon Elastic Compute, Amazon EC2, instance, you can select the AWS Region. But AWS is allowed to choose any data center in that AWS Region. However, you can choose Availability Zone if you want to. And we'll talk more about Regions and Availability Zones in a few minutes. AWS procures multiple resources for their AWS customers. So we're talking about thousands to make sure they are using economies of scale. And you will learn more about economies of scale in cost optimization. 

But for now, economies of scale allows AWS with the larger scale of resources to offer economies at scale to their customer, which means that AWS can pass along savings to their customer while also providing customers the resources needed to scale and grow. 

And this leads us to our fourth criteria that cloud computing needs, which is elasticity, which we will be talking about throughout this course. But elasticity is the ability to scale with demand. As your demand increases, you can scale up your AWS resources, and then when that demand decreases, you can scale your resources back down. 

And the fifth and last criteria that AWS must meet to be considered cloud computing is that the resource usage in the AWS accounts can be monitored and billed. So with the AWS, we are no longer pre-provisioning, procuring your storage, your servers, your compute, and so on. 

You no longer have to estimate your demands because with AWS, we are not only using services that are monitored for usage and that are billed, but we are also actually scaling those resources to meet your demand instead of being over- or under-provisioned. 

And the biggest cost optimization benefit is that we only pay for what we use with AWS. 

So how does AWS provide such amazing cloud computing? Well, AWS offers a global infrastructure, which is a collection of smaller groupings of infrastructure that is connected by a global high-speed network. And this global infrastructure allows you to design systems that are resilient and highly available. And we will dive deeper into the AWS global infrastructure later on in this course under domain 3. 

But for now, ensure you understand AWS Regions, Availability Zones, and edge locations. Let's wrap up this lesson and cover the advantages of high availability, elasticity, and agility. High availability and fault tolerance are often confused. 

So let's dive a bit deeper into both and disaster recovery too. Let's start off with high availability. 

High availability is a way to design your systems to have the ability to keep your systems up and running and providing service as often as possible. It is designed so that if a system component fails, then that failure can be replaced or fixed as quickly as possible. So it maximizes the system's online time. But high availability does not mean it stops failures. And high availability also does not mean that there will be no downtime or outages. What high availability does is respond when there is a failure to fix the failure as soon as possible so that system can be brought back into service. 

So what is an example of high availability? Well, let's say that we have an application running on a single server inside AWS. And this server is used by employees to complete their job. If this server goes down, then the employees cannot work because the server is down and it is currently experiencing an outage. If you design this architecture to be highly available, you could quickly spin up a new server to failover to or you could be running two servers for this application. One in active mode and one in a standby mode. So if one server goes down, you can failover to the second server to serve the employees. But in this situation, the employees may have to log back in so there could be a bit of downtime, which is okay. It's not what you want, but with high availability, some downtime can be expected depending on your design. The goal of high availability is to reduce outages and stay operational to be fast, and automatic recovery is best. But there is usually downtime even if it's very brief. 

Now, let's go over fault tolerance. It is similar to high availability, but fault tolerance is the actual ability of a system to keep operating in the event of a failure. For example, let's say that one or more of the system's components fails, but the system is able to continue on with those faults failing. So a fault-tolerant design must continue to operate. 

Let's go back to our earlier scenario. In this case, we are designing for fault tolerance. Then we would have two active servers serving the one application. So if one server goes down, the second server is already active and will continue to serve the employees, and there is actually no downtime in this situation. 

Fault tolerant designs operate to minimize failures, but also to continue to operate through failure. And these system designs are usually more expensive than high availability designs. 

Disaster recovery is a bit different from fault tolerance and high availability because fault tolerance and high availability are all about designing systems to operate through a disaster. Disaster recovery is all about what we need to plan for and also what we need to do in the event of a disaster. 

And having a disaster recovery plan is crucial because the worst time to recover from a disaster is in the middle of that disaster. Disaster recovery needs pre-planning and also steps to complete the disaster recovery process. So when the disaster occurs, you're already set with the plan to recover your systems as quickly as possible. 

Throughout this course, we will continue discussing high availability, fault tolerance, and disaster recovery, and how different AWS services are great options for one, both, or all. 

Let's move on and talk about elasticity. 



We mentioned scaling earlier in this lesson. Scaling is the ability of a system to scale. So increasing or decreasing the load placed upon that system. And systems scale when they need to grow or shrink depending on the load. So when a system scales, we are adding or removing resources to and from a system. 

There are two ways to scale, vertically and horizontally. An example of vertical scaling is if you have a particular size of an Amazon EC2 instance and that instance size is not able to keep up with the increase in demand, so that instance may start running very slowly, or in a worst case scenario, it may cause the system to crash. 

Vertical scaling is resizing your Amazon EC2 instances to a larger size. If you are using a T2 micro, you might need to resize to a T2 medium or larger. And by resizing your instance, you are adding more CPU and memory for your system to handle the increase in demand. And if you choose to scale vertically, you will encounter a few issues. There usually is a small disruption because the instance when resized will need to be rebooted. And to tie in cost optimization, when you vertically scale to a larger or even larger instance size, you'll also see an increase in your cost. And this is one area where you can dive deeper into the AWS Well-Architected Framework cost optimization pillar, which we will cover in the next lesson to make sure you are right sizing your instances in the beginning. 

But some benefits to vertical scaling are that there are usually no application modifications needed, and it usually works for all applications. 

Now let's talk about horizontal scaling because it was designed to fix some of the issues we have with vertical scaling. So with horizontal scaling, instead of increasing the size of your instance, you add more instances of the same size to handle that load. And instead of having one running copy of your application, you now have one copy for each instance it scaled and you can use a load balancer to help distribute the load across all of your instances. 

But we also have to consider certain things with horizontal scaling. One consideration is session. With your application, you do not want to interrupt customer sessions. When you have a single application running on a single server, then all of your sessions are stored on that server. But what do you think happens to your sessions when we scale to multiple servers with copies of your application? 

Well, sometimes you're shifted between the instances because horizontal scaling is meant to even out the load. So the load balancer sends requests to different servers that have a copy of your application. You can use the sticky sessions feature so that you do not lose your session switching between servers. And this works when scaling in or scaling out. Also, as far as for cost optimization, horizontal scaling is usually less expensive because you are using smaller, cheaper instance sizes. 

Now, let's cover elasticity. What is it? It is an important topic to know and understand for the exam. Elasticity is using automation along with horizontal scaling to match capacity and supply with the demand. You'll notice that demand is very rarely linear. It is usually increasing and decreasing. And using elasticity allows your capacity to increase and decrease and meet that ever-changing demand. 

AWS provides launch configurations and auto scaling to scale out your systems to match that capacity to that demand, which enables your environment to scale out, adding additional resources as the demand increases. And then when that demand decreases, we can scale back in to a smaller number of servers or even no servers, which optimizes for performance efficiency, operational excellence, as well as cost optimization, which are all pillars of the AWS Well-Architected Framework. 

Here are a few exam tips. Vertical scaling is a larger size and horizontal scaling is adding more instances of the same size, and adding elasticity is using automation along with horizontal scaling to match our capacity to our demand. 

Let's get started with the second task statement and talk about design principles of AWS.


Idenitfy design principles of the AWS Cloud
–
Let's get started with the second task statement, which is to identify design principles of AWS. A key focus for this task statement is the AWS Well-Architected Framework. 

For this exam, ensure you have a solid understanding of how to use the AWS Well-Architected Framework to design and build resilient, highly available and cost optimized environments. 

What is the AWS Well-Architected Framework? The AWS Well-Architected Framework is AWS's best practices and the core strategies for architecting systems in the cloud. It helps you design, build and operate reliable, secure, efficient, and cost-effective systems that will increase your likelihood of success. 

And the AWS Well-Architected Framework is based on six pillars: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. The Well-Architected Framework also identifies a set of general design principles to facilitate good design in the cloud which are to: stop guessing your capacity, use auto scaling to ensure your supply meets your demand, test systems at production scale, automate your architecture and this makes experimentation easier, allow for evolutionary changes and use data to make the needed changes and to improve through game days and run tests to see your systems at production level. 

You may be asking, "Julie, why is this important? Why are we discussing cloud computing and the Well-Architected Framework?" 

Well, the reason is that this gives you a solid basis for understanding AWS and it will benefit you to take the time to learn and understand these fundamental concepts going into your exam. AWS also provides an AWS Well-Architected Tool along with the best practices to help us too and this tool will review your workloads and compare your workloads to the latest AWS best practices for architectural design.

 And these best practices for architectural design come directly from the AWS Well-Architected Framework. I will add a link for the AWS Well-Architected Framework and also a link for the tool in case you want to take a deeper dive. 

The first pillar is operational excellence which is the ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value. There are five design principles for operational excellence: perform operations as code, make frequent, small, reversible changes, refine operations procedures frequently, anticipate failure, and learn from all operational failures. 

The second pillar encompasses the ability to protect data, systems and assets to take advantage of cloud technologies to improve your security. There are seven design principles for security in the cloud: implement a strong identity foundation, maintain traceability, apply security at all layers, automate security best practices, protect data in transit and at rest, keep people away from data, and prepare for security events. 

The reliability pillar encompasses the ability of a workload to perform its intended function correctly and consistently when it's expected to. This includes the ability to operate and test the workload through its total lifecycle. There are five design principles for reliability in the cloud: automatically recover from failure, test recovery procedures, scale horizontally to increase aggregate workload availability, stop guessing capacity, and manage change in automation. 



The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. There are five design principles for performance efficiency in the cloud: democratize advanced technologies, go global in minutes, use serverless architectures, experiment more often, and consider mechanical sympathy. 

The cost optimization pillar includes the ability to run systems to deliver business value at the lowest price point. And we will cover the cost optimization pillar under Domain 4, billing, pricing, and support. 

And the sixth pillar is the sustainability pillar. This pillar focuses on environmental impacts, especially energy consumption and efficiency, because they are important levers for architects to inform direct action to reduce resource usage. There are six design principles for sustainability in the cloud: understand your impact, establish sustainability goals, maximize utilization, anticipate and adopt new, more efficient hardware and software offerings, use managed services and reduce the downstream impact of your cloud workloads. 

And ensure you understand the best practices for all of the pillars too. The AWS Well-Architected Framework is really important in the real world, for this exam, and for the AWS Certified Solutions Architect Associate certification. 

You will most likely see questions on this certification exam asking which design principles for specific requirements or use cases would you choose when designing systems or solutions in AWS. So, take some time to read and build solutions. 

Ensure you know how to design for failures, decouple your components which reinforces the service-oriented architecture design principle, implement elasticity using auto scaling, security and parallelization which are necessary for building highly scalable applications in AWS. 

Really quickly, I want to define parallelization because I think I defined the others already. Thinking parallel is similar to decoupling, but it is how to divide a job into its simplest form and then distribute that load to multiple components to handle the demand. And an example would be breaking down large datasets into smaller chunks that can be processed simultaneously and provide faster processing times. 

And back to our first lesson, an advantage of AWS is being able to focus on services and designs, not servers and hardware as well as the fast implementation and launching of those resources too. 

Let's get started with the third task statement to understand the benefits of and strategies for migration to AWS.


Understand the benefits of and strategies for migration to the AWS Cloud
–
Let's get started with the third task statement which is to understand the benefits of and strategies for migration to AWS. For this task statement, ensure you know how to use AWS Cloud Adoption Framework and which resources can help support your migration journey to AWS. 

In our last lesson, we talked about the AWS Well-Architected Framework which is different from the AWS Cloud Adoption Framework. The AWS CAF provides a comprehensive approach to cloud and it identifies specific organizational capabilities for successful cloud transformations. These capabilities provide best practice guidance and help you improve your cloud readiness. These capabilities are grouped into six perspectives, business, people, governance, platforms, security, and operations. 

Ensure you know how to use the AWS CAF to identify and prioritize transformation opportunities, evaluate and improve your cloud readiness, and iteratively evolve your transformation roadmap. 

What are the benefits of using AWS CAF? Well, you can reduce the business risk through improved reliability, increase performance, and enhance security. You can increase your operational efficiency by reducing costs and increasing productivity. You can grow by creating new products and services to reach new customers or new markets. And you can improve performance by improving your sustainability and transparency. 

What are some cloud adoption strategies? Well, when organizations start migrating to AWS, they could be in different stages of adoption. The project stage is when you are evaluating if migrating to AWS would meet your specific requirements and needs. 

The foundation stage is when the migration to AWS begins. Maybe you move a few production applications to AWS or deploy an initial framework for the landing zone to a non-production environment. 

The migration stage is where you define roles for cloud operations, establish a cloud center of excellence, also known as a CCOE, and prepare for long-term cloud operations instead of on-premises operations. 

Last is the reinvention stage where all new projects start in AWS. An assessment of where you and your organization sits is completed to understand which stage of adoption you are at before the migration begins. 

For the exam, ensure you know how to identify appropriate migration strategies, such as database replication, using the AWS database migration service, or AWS Snowball. We will talk about these more in depth in a later lesson. 

Before the exam, know the seven migration strategies. 

Retire is for applications you want to decommission or retire. 
Retain is for applications that you want to keep in the source environment or applications that are not ready to migrate. 
Rehost, also known as lift and shift are for applications to migrate without making any changes to the application. 
Relocate is for a large number of servers that are made up of one or more applications.
Repurchase is also known as drop and shop and is for applications with a different version or product and provides more value than the existing infrastructure. 
Replatform is also known as lift, tinker, and shift and is for applications that need some level of optimization in order to operate efficiently or take advantage of AWS capabilities. 
Refactor or re-architect is for applications that you want to migrate to AWS and take full advantage of the cloud-native features to improve agility, performance, and scalability. 
What are some other AWS services and resources that are available to support your cloud migration journey? 

Let's say that you're migrating your application that stores financial information to AWS. Your application requires low latency access and the data is constantly changing. Which AWS service could you use for this application migration? Amazon Elastic File System, AWS Snowball Edge, Amazon Simple Storage Service, or Amazon Relational Database Service? 

Well, Amazon S3 is always a good choice, but for the low latency requirement, Amazon S3 is not the best choice because Amazon S3 sits in the public zone and it is located outside of your Amazon VPC it could have more latency. 

AWS Snowball Edge is used for migration when you need to move large amounts of data into or out of AWS. 

My choices would be Amazon EFS and Amazon RDS. Amazon RDS is a managed database service that provides a scalable relational database with fast performance, high availability, and security. Amazon EFS provides scalable file storage with Amazon EC2 instances, and the storage capacity is elastic too. 

What if you're migrating a NoSQL database and you must ensure the database is scalable, fast, and reliable? What AWS database service would you choose? Amazon S3, Amazon RDS, Amazon Redshift, or Amazon DynamoDB? 

Well, DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with scalability. NoSQL and non-relational are keywords for DynamoDB just as relational is a keyword for Amazon RDS. Amazon DynamoDB is a key-value and document database and stores JSON-type documents. 

AWS has another fully managed relational database service, Amazon Aurora. What if you need to launch a Microsoft SQL Server database in AWS but you need to ensure the database is cost optimized and only a standard license for SQL server is required. 

Do you launch an Amazon Aurora database that runs SQL server and buy a standard Microsoft SQL server license from the AWS License Manager service? Or do you use an RDS instance that runs SQL server with a standard SQL license? Or do you launch an Amazon EC2 instance and install SQL server and purchase a standard SQL license from Microsoft? Or do you use a Windows server with SQL Server Standard AMI so you do not need to buy or manage your own license? 

Well, AWS offers multiple Amazon Machine Images or AMIs for Amazon EC2 instances. If you launch an Amazon EC2 instance using a Windows AMI, you can choose one bundled with SQL Server Standard, and you do not need to purchase your own license from Microsoft. 

You could possibly choose the Amazon RDS database instance that runs SQL Server Standard and purchase the license, but the question asked for cost optimization and Amazon RDS costs more than Amazon EC2 because Amazon RDS is a managed service. 

Also, Amazon Aurora only supports MySQL and PostgresSQL, not SQL Server. Another area of focus for this task statement is data backups because you need a plan in place to store your data once migrated. 

Ensure you understand the cost storage options for the AWS storage services. For example, Amazon S3 Glacier storage classes are designed to be the lowest-cost Amazon S3 storage classes allowing you to archive large amounts of data at a very low cost. But you do have to consider the retrieval fees and the retrieval times. 

Let's get started with the fourth task statement and talk about cloud economics.


Understand the concepts of cloud economics
–
Let's get started with the fourth task statement, which is to understand concepts of cloud economics. Ensure you know how using AWS can help you shift your technical resources away from on-premises infrastructure management, such as server acquisition and maintenance, space management, cooling, and everything else that goes along with running and collating your data center requirements. 

We mentioned this in an earlier lesson. The AWS Well-Architected Framework has design principles and recommends you adopt the consumption model and only pay for what you consume. Stop spending money on data centers and data center operations, measure your overall efficiency, and analyze and track your cost and usage, and then use AWS-managed services to reduce the overall cost of ownership. 

Because remember, AWS offers a lower cost because of economies of scale. When you stop spending money on data centers and pay for what you consume, your technical resources are freed up and are available to prioritize other activities. 

Activities such as optimizing resource utilization, creating more efficient applications, developing better end-user experience, and more areas that will help your organization streamline operations and generate more revenue. 

For the exam, it is also important to understand how to operate in AWS partially but also fully because either can affect the ownership and operational cost for your organization. There are four parts of total cost of ownership, or TCO, for AWS to focus on for this exam. 

Operational expenses are OpEx, which are your day-to-day operating costs such as utilities, printer toner, coffee, and snacks. 

Capital expenses or CapEx, which are the costs associated with creating the longer-term benefits such as purchasing a building, servers, your printer that uses the toner mentioned before, and your power backups. CapEx items are generally purchased once and are expected to aid your organization for years. 

Labor calls for staffing an on-premises environment or data center, such as the network operation center technicians who might handle the installation, configuration, troubleshooting, and management of your servers and infrastructure, and also the impact of software licensing costs when moving or migrating to AWS. 

It's important that you understand how any software licenses you are currently using might be affected by a move to AWS. Can they be transferred or do they require different kinds of license? Can you forego your current license in favor of one managed by AWS? But all of these and other questions should be considered when evaluating the economics of operating in the cloud. Do you know which operations will reduce costs by moving to AWS? 

It's not usually a one-to-one transition. Some applications can be moved directly to AWS, but there are often additional things that need to be considered to be most economical. 

Do you know how to establish benchmarking and performance testing instead of focusing on provisioning for peak demand? Automation is also important and can help reduce costs. Like we mentioned in an earlier lesson, there could be a cost savings to scaling horizontally to meet your demands instead of running on premises and running at peak capacity when you're not in peak demand. 

Also, look at using data segmentation and targeted reporting that can also help you to reduce the scope of your compliance and save time during audits. And consider how strongly the use of managed services can help reduce your technical workload and costs for a variety of different use cases. 

For the exam, dive deeper into right sizing, adding automation, ensuring compliance is met and reducing that scope, and using AWS Managed Services, which are all covered in the AWS Well-Architected Framework. 

Exam tip. When you migrate from an on-premises environment and traditional servers to AWS, you are trading capital expenses for variable expenses. 

We'll be diving deeper into AWS services in Domain 3, cloud technology and services. I know I've been mentioning various service, benefits, tools and features to meet different requirements and use cases, and it is a lot to remember and understand, but as we go through this course, we will continue to identify keywords for different services and try to take the topics and concepts and build on each of them. 

The deeper you dive into fundamentals, the benefits of AWS and cloud economics, the more it will all make sense for the certification exam and the real world. 

Let's get started with our first walkthrough question.


Walkthrough question 1
–
Welcome to our first walkthrough question for this course. These walkthroughs are meant to assist you in a few ways. 

First, this will be one of the opportunities to see the types of questions that you'll encounter on the actual exam. While the questions aren't pulled directly from the certification exam, they are of similar quality and difficulty, and will give you exposure to this style of questioning. 

Second, I wanna show you methods I consider to be helpful when you're working with multiple-choice questions. These methods help you focus on what you're looking for and help you identify any distractors you may encounter on your exam. 

And third, these questions will provide you with additional information. Any questions you feel confident in will reinforce your knowledge in that area, and any questions that reveal gaps in your knowledge will help you identify where to focus in your studies going forward. 

As I go through each of the questions, I'll generally follow a particular format. I'll start by reading through the question. Then I'll identify keywords and phrases in the question that show you exactly what you're looking for in the responses, or answers. After that, I'll go through the responses and give you time to figure out if you can identify the correct response yourself. 

After you've been given a chance to figure it out yourself, I'll go through the responses and discuss why they're correct or incorrect. Okay, now that I've given you that background information, let's get started with the question which is from task statement 1.2: Identify design principles of the AWS Cloud. 

The question reads: "Which AWS Cloud architecture design principle supports the distribution of workloads across multiple Availability Zones?" 

Reading this question, can you identify any keywords or phrases? And also, what exactly is the question asking? A few keywords I see are design principles and distribution across multiple Availability Zones. 

Now that we have examined the question, identified some keywords, and reviewed the requirements, let's explore the responses. 

Option A, Implement automation. 
Option B, Design for agility. 
Option C, Design for failure. 
And Option D, Implement elasticity. 
Pause the video if you need more time. Okay, let's evaluate the options. 



Option A is incorrect. You can use automation services, such as AWS CloudFormation, to deploy resources into one or more Availability Zones. However, the implementation of automation is not directly tied to, or limited to, the distribution of workloads across multiple Availability Zones. 

Option B is incorrect. When you design for agility, you can provision resources more quickly. Agility is not related to the number of Availability Zones. 

Option C is to design for failure. AWS recommends that you distribute workloads across multiple Availability Zones. This distribution will ensure continuous availability of your application, even if the application is unavailable in one single Availability Zone. This makes option C a good candidate for the correct answer, but let's look at the rest of the responses. 

Option D is also incorrect. Elasticity is the ability to activate resources as you need them and return resources when you no longer need them. Elasticity is not related to the number of Availability Zones. 

Option C is the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question. And let's get started with our second walkthrough question.


Walkthrough question 2
–
Let's get started with our second walkthrough question, which is from task statement 1.3, understand the benefits of and strategies for migration to the AWS Cloud. 

The question reads, "A system administrator is reviewing a group of servers that were found during a portfolio discovery. All servers are migrating to AWS. The servers have no current owner. There is very little traffic to the servers. Which migration strategy should the system administrator suggest for these servers?" 

Reading this question, can you identify any keywords or phrases and exactly what this question is asking? A few keywords I see are servers, portfolio discovery, migrating to AWS, and little traffic. Also, the final ask is looking for a migration strategy for the found servers. 

Now that we've examined the question, identified keywords, and reviewed the requirements, let's explore the responses. 

Option A, rehost
Option B, replatform
Option C, retain
Option D, retire
Pause the video if you need more time. Okay, let's evaluate the options. 

Option A is incorrect. To re-host a server means to move the server to the AWS Cloud without making any changes to the server. Because the servers are rarely used, the servers can be shut down to save on cost. 

Option B is incorrect. To re-platform a server means to move the server to the AWS Cloud and make changes to optimize the server. Because the servers are rarely used, the servers can be shut down to save on cost. 

Option C is also incorrect. To retain a server means to keep the server in the current environment without moving the server to the AWS Cloud. Because the servers are rarely used and all servers are being migrated to AWS, the servers must be shut down. 

So that makes option D correct. To retire a server means to decommission or remove the server from the environment. Because the servers are rarely used, the servers should be retired. The cost to move and keep the servers running may be more than the productivity that the servers provide. By retiring the servers, you can realize savings upfront by no longer having to pay for those servers. 

That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question, and let's get started with domain 2, security and compliance.

Domain 2: Security and Compliance

This module has seven videos.


Domain 2 Introduction
–
Let's get started with Domain 2, which covers security and compliance. This domain covers the AWS shared responsibility model, security, access management, and compliance. 

One key aspect of demonstrating your understanding of AWS is being able to recognize who is responsible for what, in regards to security. 

In AWS, some of the technical details about how services are hosted or how data is stored can be abstracted from you, but it doesn't mean that you are not responsible for security in AWS. You should be able to identify when security measures need to be put in place, and when security measures are already put in place by AWS for you. 

Domain 2, security and compliance, is broken into four task statements that we'll discuss over the next few lessons. 

Task statement 2.1: understand the AWS shared responsibility model. 
Task statement 2.2: understand AWS Cloud security, governance, and compliance concepts. 
Task statement 2.3: identify AWS access management capabilities
Task statement 2.4: identify components and resources for security. 
For the first task statement, we will focus on the AWS shared responsibility model. Being a cloud practitioner means you need to be able to determine when you need to secure resources within AWS and the depth to which you need to be involved for security, which can vary from service to service. 



For the second task statement, we will focus on understanding security, governance, and compliance concepts. 

For the third task statement, we will focus on access management in AWS. We just mentioned that the level of responsibility the customer assumes changes, depending on the service they're using, but you are always responsible for access management to your environment. 

For the fourth task statement, we will dive deeper in how to secure the different AWS services. You don't need to know the step-by-step directions on exactly how to enable security within AWS for this exam, but you do need to know generally what security features exist, and when you are responsible for enabling them. 

Over the next several videos in this module, I'll address each task statement individually, breaking down the knowledge and skills expected of you to be successful. 

Let's get started to evaluate your readiness for the exam in the next video, where we will cover the first task statement from Domain 2, which is to understand the AWS shared responsibility model.


Understand the Shared Responsibility Model
–
Let's get started with the first task statement for domain 2, which is to understand the AWS shared responsibility model. In domain 1, we discussed AWS as cloud computing and also introduced the AWS global infrastructure. 

In this lesson, we will take it a step farther and discuss the AWS shared responsibility model. So what is the AWS shared responsibility model? 

Well, the AWS shared responsibility model is how AWS provides clarity around which areas of system security belongs to AWS and which belongs to the customer. AWS provides the shared responsibility model so you are clear about which elements of the infrastructure AWS manages and what elements customers are responsible for managing. 

At a very high level, AWS is responsible for the security of the cloud, but you are responsible for the security of your data, applications and so on in the cloud. If you look at our diagram with the AWS global infrastructure on the bottom, you'll also see different service levels on top of that global infrastructure. 

There are Regions, Availability Zones, and edge locations around the world. And on top of that, we have the AWS global infrastructure. AWS manages all of this, the hardware, the security, and more for this global infrastructure. And you, as the AWS customer, have no control over any of this and you don't have to worry about this infrastructure. 

Now, we can add in the next level of the global infrastructure, and that is the levels of compute, storage, networking, databases, and so on. AWS is again responsible for managing these and the security of these services and systems. 

There is another level that AWS manages and that is the software that assists any of the levels of services below it. So AWS manages the Regions, the global infrastructure, the hardware, the network, and the level services like compute, databases, network, storage, and more, and also any software that is used to provide these services. 

For example, if you choose to provision an Amazon EC2 instance from the AWS compute level, AWS manages the Region and the Availability Zone that your Amazon EC2 instance will run in. AWS handles the provisioning and the security, compute, networking, and storage needed for this instance and also handles the software for the instance such as the user interface or the hypervisor. 

Now, this is where your responsibility comes in. You are responsible for the operating system and upwards. So that would be things like client-side data encryption, integrity, authentication, server-side encryption, and protecting the network traffic. And that includes encrypting your data using SSL certificates and so on. 

You are responsible for all of this as well as the operating system and the network and firewall configurations. You are responsible for your application and your identity access management, so you allow people to have certain access to your account and the services in that account. You are also responsible for your customer data, securing that data and definitely backups of that data. 

Being a cloud practitioner means you should be able to determine when you need to secure resources in AWS and the depth to which you need to be involved for security which can vary from service to service. Another example is who is responsible for securing the data centers that host AWS services? In this case, the answer is AWS and not the customer. 

This is the level of determination you need to be able to make regarding the AWS shared responsibility model. You should also be able to describe how a customer's responsibility shifts depending on the service they are using. 

For example, let's say you're hosting a MySQL database on Amazon RDS, are you responsible for patching the database engine on an Amazon RDS database instance or is AWS responsible for this security patching? What if you have the same database running on an Amazon EC2 instance? Who would be responsible for patching in that use case? 

The answer to these two questions is that AWS is responsible for patching on Amazon RDS whereas you, the customer, are responsible for patching on Amazon EC2. The task you are responsible for with regards to security depends on if an AWS service is managed or not. In the previous example, Amazon RDS is a managed service so you are required to carry out less of the security and management tasks in general. However, Amazon EC2 is unmanaged, meaning you have more control over the service and are therefore more responsible for carrying out security and management tasks. 

The key takeaway here is the level of responsibility the customer assumes changes depending on the service they're using. It is important to know for the exam what the customer versus AWS is responsible for for services such as Amazon RDS, Amazon EC2, Amazon DynamoDB, and AWS Lambda just to name a few. 

Let's get started with the second task statement and talk about cloud security governance and compliance.


Understand Cloud security, governance, and compliance concepts
–
Let's get started with the second task statement to understand AWS Cloud security, governance, and compliance concepts. When you're working with AWS, you need to ensure security and compliance are considered when making design decisions. 

We mentioned this earlier under domain 1 when we talked about choosing AWS Regions to understand compliance needs among geographic locations and industries. AWS compliance helps you understand the controls in place at AWS to maintain security and data protection in AWS along with assurance programs that provide templates and control mappings to help customers establish the compliance of their environments running on AWS. 

Even if you're not the person who would actually be building the solutions on AWS, you should be aware of how to find information about security and compliance and what services exist to support a security and compliance plan. The first thing you should know about compliance on AWS is where to find compliance information. 

We just mentioned AWS Compliance programs. Let's say you are building out a solution on AWS that requires a General Data Protection Regulation or GDPR compliant status and you want to use the service Amazon DynamoDB to store your data. How do you know if DynamoDB is GDPR compliant? Where could you find this information? You can find compliance information through the service AWS Artifact which gives you on-demand access to the AWS security and compliance documents and reports. 

Now, notice how the question I asked was about how to find compliance information for DynamoDB. I didn't ask, "Is it GDPR complaint?" There is a reason for this. Each individual AWS service has differences in regards to compliance. One service may require one set of actions to be taken to be compliant with a compliance standard whereas another AWS service would require a different set of actions to be taken to reach the same compliance standards. 

So, am I saying you need to memorize every single compliance program and which AWS services meet those requirements? Luckily, that level of detail won't be on the exam. Instead, you should understand where to find compliance information and be able to identify that compliance requirements do vary from service to service. 

Beyond recognizing where to find compliance information, you should also be prepared to answer questions around the different ways compliance and security can be achieved on AWS from a high level. When you are thinking about compliance and security, the main thing you are trying to do is protect systems and information. How do you secure resources on AWS? 

What if you want to add more security to your network beyond security groups and network access control lists? What other AWS service can you use? AWS WAF and Amazon GuardDuty are AWS security services. GuardDuty is a threat detection service to monitor for malicious activity and unauthorized behavior. AWS WAF is a web application firewall to help protect your applications from common exploits that could impact your application availability, compromise your security or consume excessive resources. 

What AWS service helps to protect your resources from common distributed denial of service or DDoS attacks? AWS Shield is a managed DDoS protections service to help safeguard your applications running on AWS. There are other security services too such as Amazon Inspector, AWS Security Hub, and more. 

Another way to secure data on AWS is through encryption. Encryption is a definite fundamental you need to know for this certification and the real world. Let me know if you want to take a deeper dive, I have some encryption fundamentals I can share. 

Encryption isn't a singular thing that you do one time. There are many different ways you can use encryption techniques. For example, there is encryption of data in transit and encryption of data at rest. Data in transit refers to data as it moves between two different places and data at rest refers to stored data. 

As a cloud practitioner, you should know the difference between encrypting data in transit and at rest because this will help you select AWS services that support the level of encryption you need. In regards to encryption, you should also be able to identify who enables encryption for different AWS services, which leads us back to the AWS shared responsibility model, but also requires AWS service specific knowledge. 

You can find information on encryption in the AWS whitepapers and documentation too if this task statement is something you aren't familiar with. Another part of working with AWS is being able to identify services that enable you to log, audit, and create reports on the activity happening in your AWS account. 

Activity in an AWS account could be AWS users creating and modifying AWS resources or it could be an external user accessing and using your AWS resources, like sending a request to a backend web server for example. 

You should understand what logs are and that they can be used to troubleshoot or audit the activity in your AWS account. You should be able to make distinctions between the services like Amazon CloudWatch which is used for monitoring and collecting operational data versus AWS CloudTrail which is used to log events related to AWS resource creation and management versus AWS Config which is used to take inventory of current configurations and audit resources to ensure they maintain correct configurations. 

What AWS service do you choose to identify an IAM user who deleted an Amazon EC2 instance in your production environment? CloudTrail is a service for governance, compliance, operational auditing, and risk auditing of your AWS account that continuously monitors and retains account activity related to actions across your AWS infrastructure. 

Let's dive deeper into CloudTrail. What is the best way to keep track of all of your activities in your AWS account with CloudTrail? You can create a CloudTrail trail for multi-Regions. There is also another service, AWS Audit Manager. 

You don't need to understand the logs or data that these services create, but you should be able to explain what the services do, explain their use cases, and understand the difference between them. One final point for this task statement is that not every person using an AWS account should have the same level of permissions. Instead, you should be following the concept of least-privileged access where you only give people the level of access they need and nothing more. 

Let's get started with the third task statement and talk about access management.


Idenitfy AWS access management cpabilities
–
Let's get started with the third task statement to identify AWS access management capabilities. There are a few different things you should make sure you understand and can explain for this task statement. 



First, you should understand the need for user and identity management. So, why do you need to have tools to control user access? We mentioned earlier that not everyone who uses an AWS account needs the same level of access, and in order to control how people use AWS services within an account you need a way to control the level of access they have. 



For example, someone who is an analyst likely won't need administrator access for the service Amazon EC2 like a systems administrator may need. You should familiarize yourself with the concept of principle of least privilege, where you only give users exactly the level of access, they need to do their work and nothing more. 



Security is another fundamental needed for this exam along with user and identity management, it's all about knowledge of AWS Identity and Access Management. You should know the features of AWS IAM, how it enables you to control access to your AWS account, and when to use the different aspects of IAM based on the use case. 



A fundamental understanding of AWS accounts is needed for this exam. AWS accounts may seem like a very basic understanding but what accounts are and how they work are crucial because as a cloud practitioner and as a solutions architect you will encounter very simple systems operating in a single AWS account, but you will also encounter more complex systems that use tens or even hundreds of AWS accounts. 



An AWS account is where you provision your services, it is also where your AWS services log their usage to and also with an AWS account you can log into that environment. And these services you provision in your account generate a bill and that is billed to you from the payment method you choose when you create your account. 



Remember that is the fifth criteria of cloud computing we covered in our first lesson. It is also important to understand that when you create your AWS account, it is yours. And only yours until you grant access to others. 



AWS accounts by default do not have permissions until you explicitly grant those permissions. When you create an AWS account, you begin with a sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. 



For the exam, you need to be able to explain how the root user is different from other types of users within the AWS account. The root user has complete and unrestricted access to all resources in an AWS account, but you should not be using this user to carry out daily tasks in AWS. Because of this unrestricted access, it's incredibly important that every cloud practitioner knows how to protect root users, when to use the root user, and when not to use the root user. For the exam, ensure you have studied the different ways you can lock down your AWS account root user to protect it. This includes the use of multi-factor authentication, securely locking away the root user credentials, rotating access keys and the password for the root user, and simply not using this root user for daily tasks but instead AWS recommends that you configure an admin user in AWS IAM Identity Center to perform daily tasks and access AWS resources. 



There are a limited number of tasks that require root user access, and you should be familiar with what those tasks are. For the exam, know which tasks you can perform as the root user of an account. You can: change your account settings, restore IAM user permissions, activate IAM access to the Billing and Management console, and more. You can also create an account alias in IAM which substitutes the account ID in the web address for your account for IAM users. 



Secondly, you should know the different features AWS IAM has to offer and understand when you would use them. The features I am referring to are: IAM users, groups, roles, and policies. You should be able to explain things like how IAM users have associated usernames and passwords, access keys for programmatic access, how users can have MFA enabled for their login, and how you can enforce password complexity and access key and password rotation. 



Know how to organize users into groups and how that affects permissions when AWS actions are taken. You need to know how permissions in AWS work and how you can apply IAM policies to users, groups, or roles and the impact that has on an entity's access. 



I mention IAM roles, and it's something you should be very familiar with before attempting the exam. Roles are temporary credentials that can be assumed by various entities and have many use cases. For example, some use cases are: a user can assume a role to gain temporary access to permissions, a program can assume a role to gain access to AWS credentials to make AWS API calls, or you can use roles for cross-account access. 



Do you know when it is better to use an IAM role rather than an IAM user? IAM roles can be used to provide AWS services permissions to do certain actions and also to give outside entities permissions to perform actions in your AWS account. 



Here is another question. What AWS service can you use to provide temporary AWS credentials for users that have been authenticated through their social media logins and guest users who do not require authentication. Amazon Cognito and specifically Amazon Cognito Identity Pool can provide temporary AWS credentials for this use case. Diving deeper, Amazon Cognito User Pool is a user directory in Amazon Cognito and it doesn't enable access to unauthenticated identities. You have to use an Identity Pool instead. 



Third, understand IAM policies. There are different types of IAM policies in AWS, managed policies and unmanaged. You should know the difference between the two types of policies, who can create each type of policy, and who can edit or modify each type of policy. 



AWS creates and manages managed policies whereas customers create and manage regular IAM policies. Related to IAM policies, there is an AWS service you can use to test and troubleshoot IAM and resource-based policies. Do you know what that service is? It is the IAM Policy Simulator. 



Also, what if you need to add security to your Amazon S3 bucket to only allow access for specific users, do you create an IAM role, IAM user policy, or a bucket policy? Well, bucket policies are a type of resource-based policy and they differ from IAM roles. Bucket policies and user policies are two of the access policy options available for you to grant permission to your Amazon S3 resources. Both use JSON-based access policy language. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in the bucket. User policies are policies that allow an IAM user access to one of your buckets. 



Let's stay on Amazon S3 and see if you can answer this one. If one of your objects was accidentally deleted from your S3 bucket, what can you implement to prevent unauthorized deletion of any other objects from your bucket? You can configure MFA delete on your S3 bucket which adds an extra layer of protection, but you must enable versioning for your objects first. 



Once you understand AWS accounts it is easier to understand how to manage and secure access. 



Let's get started with the fourth task statement and talk about components and resources for security.


Idenitfy components and resources for security
–
Let's get started with the fourth task statement to identify components and resources for security support. An important thing to remember is as a cloud practitioner it's likely you yourself will not be building out security solutions. But it is likely that you will be guiding people towards solutions that meet their needs. 

And in order to do this you must know what security services exist, and how to find more information on them. Take network security, for example. You'll need to be able to answer questions about the basic functionality for AWS security services like security groups, network access control list, and AWS WAF. 

You should understand the use cases for these services, and the differences between them. Again, you do not need to know the specifics around how these work in-depth, but you should know enough to be able to answer questions about when to use each one, and their function. 

For example, what can you use to secure your Amazon VPC subnets? Network access control lists act as a firewall to control traffic traversing your subnet. Know the difference between network access control lists and security groups. Network access control lists are used for traffic entering or leaving a subnet, because network access control lists are associated with the subnet, not with the resources. Network access control lists only manage traffic that is crossing that subnet boundary. 

If you have two Amazon EC2 instances in your Amazon VPC that are communicating, network access control list will have no involvement because the communication between the two instances are not crossing the subnet boundary. Network access control lists are stateless, in which they only see traffic going one way. So if you allow an inbound rule, then you must also allow an outbound rule so your network access control list will explicitly see that that traffic that was allowed inbound is also allowed out. Network access control lists see the traffic as two separate, different streams, so you must have two rules. One rule for each stream. If you do not add your outbound rule, then the traffic will only be allowed in. 

Security groups secure your resource level network such as your Amazon EC2 instances or Amazon RDS instances. They do not operate at the subnet level. They actually operate at the resources Elastic Network Interface level. Security groups also have inbound and outbound rules, but they are stateful, meaning that if the traffic is allowed in then that traffic is automatically allowed back out. Security groups see both the inbound and outbound traffic as part of the same stream. 

One big difference between security groups and network access control lists is that security groups recognize AWS resources. You can add rules for other security groups, or add a rule for the security group themselves. And another is that security groups have a hidden explicit deny, and that means that anything that's not explicitly allowed is denied. 

For the exam, make sure to understand security groups cannot explicitly deny. If you need to explicitly deny, then you need to use a network access control list. Security groups also accept rules with IP addresses, IP address ranges, and again, the security group IDs as a source or destination for the inbound and outbound rules. 

Let's continue on talking about AWS security services. What AWS service would you choose if you need to create rules to filter web traffic based on conditions such as IP addresses, HTTP headers, or custom URLs? AWS WAF helps to control traffic with rules that you define that block common attack patterns, such as SQL injections or cross-site scripting. 

Here's another question, can you conduct security assessment and penetration testing without prior approval against your AWS resources? Yes, but only for certain services. There are also some AWS services, like AWS Trusted Advisor or Amazon Inspector, that can give you the recommendations around security, and you should be aware of that for the exam. But you should also recognize sometimes the solution to your security and compliance needs could be third-party software or tools. 

So, where can you find available third-party software that you can deploy in your AWS account? The answer is the AWS MarketPlace. Knowing what exists at a high level for AWS services versus the types of solutions you can find via the AWS Marketplace is important knowledge for the exam. 

It is also likely when working with AWS you'll need to be able to identify resources to dive deeper into cloud topics when necessary. For security and compliance there are a few different places you should be comfortable with when looking for information or doing research. The AWS Knowledge Center is one place to find answers to your questions, and the Security Center is another place for information related to security in AWS. 

You should also be comfortable reading AWS security blogs, and working with the AWS Security Forum to find information. If you're feeling like you need to brush up on security before your exam, check out the AWS documentation for best practices, the whitepapers for deeper dives, and the AWS official documentation for general information, use cases, and information on what services exist, and how they work. 

Let's get started with our third walkthrough question.


Walkthrough question 3
–
Let's get started with our third walkthrough question, which is from task statement 2.1, understand the AWS shared responsibility model. 

The question reads, which task is the responsibility of the customer according to the AWS shared responsibility model? 

Reading this question, can you identify any keywords or phrases and also exactly what the question is asking? A few keywords I see are the shared responsibility model and the responsibility of the customer. 

Now that we've examined the question, identified keywords, and reviewed the requirements, let's explore the responses. 

Option A, install patches on an Amazon RDS database instance. 
Option B, patch the operating system of Amazon RDS database instances. 
Option C, determine which services have access to an Amazon DynamoDB table. 
And Option D, patch the Amazon VPC network devices. 
Pause the video if you need more time. Okay, let's evaluate the options. 



Option A is incorrect. AWS provides Amazon RDS as a service. AWS manages patches for the Amazon RDS engine. The customer can choose a time window to apply patches. 

Option B is incorrect. AWS provides Amazon RDS as a service. The customer is not responsible for patches to the operating system. 

Option C is to determine which services have access to the table. The user determines access permissions between services within the cloud. This makes option C a good candidate for the correct answer but let's look at the rest of the responses. 

Option D is incorrect. AWS manages network devices that provide AWS network services. AWS also installs patches. 

So that makes option C the correct answer. That's all for this question. 

Be sure to take note of any knowledge gaps that you may have identified while exploring this question, and let's get started with our fourth walkthrough question.


Walkthrough question 4
–
Let's get started with our fourth walkthrough question, which is from task statement 2.3, identify access management capabilities. 

The question reads: "A company has an application server that runs on an Amazon EC2 instance. The application server needs to access contents within a private Amazon S3 bucket. What is the recommended approach to meet this requirement?" 

Reading this question, can you identify any keywords or phrases and exactly what the question is asking? A few keywords I see are application server, EC2 instance, and access is needed to a private S3 bucket. 

Now that we've examined the question, identified keywords, and reviewed the requirements, let's explore the responses. 

Option A: create an IAM role with the appropriate permissions. Associate the role with the EC2 instance. 
Option B: configure a VPC peering connection to allow private communication between the EC2 instance and the S3 bucket. 
Option C: create a shared access key. Configure the EC2 instance to use the hardcoded key. 
And option D: configure the application to read and access key from a secured source. 
Pause the video if you need more time. Okay, let's evaluate the options. 

Option A is to create an IAM role. IAM roles are temporary credentials that expire. IAM roles are more secure than long-term access keys, because they reduce risk if credentials are accidentally exposed. This makes option A a good candidate for the correct answer, but let's look at the rest of the responses. 

Option B is incorrect. A VPC peering connection is a network connection between two VPCs that enables you to route traffic between them by using private IP version 4 addresses or IP version 6 addresses. VPC peering connections cannot connect a VPC and an Amazon S3 bucket. 

Option C is incorrect. The creation of a shared key is not a best practice, because this reduces the value of auditing AWS resource access. It is not a best practice to embed access keys into the application code, because application code can can become compromised. 

Option D is also incorrect. It is a best practice to use IAM roles instead of long-term access keys. Long-term access keys, such as those associated with IAM users and AWS account root users remain valid until you manually revoke them. However, temporary security credentials that are obtained through IAM roles and other features of AWS security token service expire after a short period of time, using temporary security credentials to help reduce the risk in case credentials are accidentally exposed. 

This makes option A the correct answer. That's all for this question. 

Be sure to take note of any knowledge gaps that you may have identified while exploring this question and let's get started with domain 3, cloud technology and services.

Domain 3: Cloud Technology and Services

This module has eleven videos.


Domain 3 Introduction
–
Let's get started with Domain 3 covering cloud technology and services. With cloud concepts and security and compliance already covered in earlier lessons, now we will dive deeper into the AWS services and technology and talk about types of AWS services, the infrastructure, and the operational aspects of using the AWS Cloud. 

Domain 3: Cloud Technology and Services is broken into 8 task statements that we will discuss over the next few lessons. 

Task statement 3.1: Define methods of deploying and operating in the AWS Cloud. 
Task statement 3.2: Define the AWS global infrastructure. 
Task statement 3.3: Identify AWS compute services. 
Task statement 3.4: Identify AWS database services. 
Task statement 3.5: Identify AWS network services. 
Task statement 3.6: Identify AWS storage services. 
Task statement 3.7: Identify AWS artificial and machine learning services and analytics services.
And task statement 3.8: Identify services from other in-scope AWS service categories. 
For the first task statement, we will cover the methods of deploying and operating in AWS, focusing on types of deployment models, and connectivity options. 

For the second task statement, we will dive a bit deeper into the AWS global infrastructure and components that make up our global infrastructure, such as the Availability Zones, Regions, and edge locations. 

For the third task statement until the eighth and last task statement, we will talk all about AWS core services. I'll talk about the categories of core services, and then give some examples of each. There are 4 categories of services when discussing the core AWS services: Compute, Storage, Networking, and Database, but we will also cover artificial intelligence, machine learning, analytics, and a few other AWS services that are in scope for this exam. Ensure you know how various AWS services fit into the different categories of compute, storage, networking, and more. What makes a service compute versus database, and what does that say about the services? Is there any overlap you should be aware of? Next, be aware of why you would choose one category and service over another. When would you choose a database service over running a database on an Amazon EC2 instance? And last, pay attention to what sort of subcategories exist within each of the separate larger categories of the services. We will cover all of this. Over the next several videos in this module, I will address each task statement individually, breaking down the knowledge and skills expected of you to be successful. Let's start to evaluate your readiness for the exam in the next video where we will cover the first task statement from Domain 1: Define the methods of deploying and operating in AWS.


Define methods of deploying and operating in the AWS Cloud
–
Let's get started with the first task statement to define methods of deploying and operating in AWS. 

There are various methods that can be used to communicate to AWS for the provisioning and necessary operations you might need to manage. Methods available are programmatic access to the application programmatic interfaces, or APIs, through the software defined kits, or SDKs, API access through the AWS Command Line Interface, or CLI, graphical interface through the AWS Management Console, and the ability to create and deploy through various infrastructure as code offerings. 

Not only is it important for you to know what methods exist but you should also be aware of the strengths and limitations of each. Think about when you would use the Management Console over the software development kits, or the Command Line Interface versus something like CloudFormation, which is the AWS infrastructure as code service. 

What benefits do you see in accessing a service like Amazon S3 in the console or running the commands in the CLI? It's one thing to know that different methods exist but it's more important to know how and when you would use them. 

What about cloud deployments? Models such as cloud native in AWS or hybrid in on-premises, similar to the way of provisioning and operating, it's not only important that you know how to define these different deployment types, but that you are also aware of how they would look in action. 

Would applications working together between your local data center and AWS be considered an on-premises or hybrid deployment? Why would you choose to keep certain resources on-premises instead of moving everything directly into AWS? 

What are some considerations that you need to note when utilizing the various types of deployment? These are some questions that you should be asking yourself when trying to understand the cloud deployment models. 

Let's dive a bit deeper. There are different models to deploy and operate in AWS. There is a public cloud, private cloud, hybrid cloud, and multi-cloud. Ensure you know the differences for the exam and the real world usage too. We already talked about cloud computing so now let's talk about the different types of cloud. Let's start with the public cloud. 

What do you think makes a public cloud environment? Well, it is simply a cloud environment that is available to the public that meets the cloud criteria that we discussed earlier in our cloud computing lesson. And as we know, AWS offers a public cloud platform just like Azure and also Google. And this leads us into the multi-cloud environments. So what do you think a multi-cloud environment looks like? 

Well, it is still the public cloud but you are using more than one public cloud. So maybe you're using AWS and Azure, or AWS and Google, or maybe you're using all three, which sounds really cool but it can be difficult to design and build. 

Next, we have private cloud. Private cloud has dedicated services for your environment, and those services are actually on-premises. So these services are dedicated directly to your environment, and each cloud vendor has a private cloud service such as AWS Outposts. 

The main difference between having other services on-premises and having a private cloud on premise is that the private cloud still has to meet the cloud computing criteria we already discussed but also be directly dedicated to your business. 

And then that leads us into a hybrid cloud, which is using the public cloud and the private cloud together. Some people think a hybrid cloud is using AWS in your on-premises data center, but it's not. 

If you use AWS and your on-premises data center together, that is actually a hybrid environment. The difference is there, and it is easy to get the two confused, but a hybrid cloud model is public and private cloud meeting the cloud computing criteria, and a hybrid environment is using your on-premises data center and AWS. 

The exam, similar to the real world, will require more of you than just a textbook definition. 

Let's dive a bit deeper and look at the architecture for public and private AWS services. What do you think AWS means when they label a service as public or private? 

Well, it is probably not what you're thinking. AWS determines whether a service is private or public by its network. And this is something I really struggled with when I was starting out. 

So what is an AWS public service? Well, it is a service that can be connected to from anywhere there's an internet connection. So if we look at this diagram, inside of AWS, we have an AWS public zone. And this public zone can communicate openly with the public internet. And AWS public services sit inside this public zone. This is not the public internet. Remember, AWS has its own network and the AWS public zone sits inside AWS but it sits adjacent or sits beside the public internet. 

And inside of AWS and inside of the AWS public zone is a network that is attached directly to the public internet. So any AWS service that is considered a public service sits and runs from this public zone with the network attached to the open internet. And an example of an AWS public service is Amazon S3. 

So when users are connecting to an Amazon S3 bucket, they are connecting through the public internet. 

Now, what is an AWS private service? Well, it is a private AWS service that lives inside the AWS private zone. And from this diagram, you can see it has no direct connection to the open internet or to the AWS public zone. And inside the AWS private zone, you can create an isolated environment like an Amazon VPC or create an Amazon EC2 instance, and by default it is completely isolated. 

But you can add permissions for your Amazon EC2 instance or for your Amazon VPC to access the public zone or the public internet. We simply have to configure this routing or make the instance public, and we'll be going much deeper into that later on in this course. And by default, the only services that can access the services inside the private zone are the services inside the private zone. 

Also, by default, there are no permissions for the private zone except for your local routes. As we go through this course, we'll be discussing if AWS services are private or public, and you may see some exam questions asking how to give a user or customer access to a specific AWS service. 

And if you understand what makes a service a public AWS service or a private AWS service and how to design that connectivity, then you will be able to confidently answer any questions you may see. 

This task statement also covers the connectivity options. The focus here is how network connectivity can be handled to various AWS services, and include creating a virtual private network, or VPN, setting up Direct Connect, and using the public internet. You should be able to identify the advantages and limitations of each of the connectivity types, understand use cases where one might be preferred or avoided, and know the generalities of how they work. 

For example, AWS Direct Connect connections are a private dedicated connection, and dedicated connection is a keyword for Direct Connect. Here's a question. What do you use to connect public Amazon EC2 instances in a public subnet to the public internet? An internet gateway or a NAT gateway? 

An internet gateway is a horizontally scaled, redundant, and highly available gateway to allow communication and traffic between instances in your VPC and the public internet. You're expected to know how these connectivity, deployment, and operational methods can be utilized, and when you would choose one option over another. 

And as you continue through the technology domain, keep in mind that it's more than just definitions. Envision how you would use the components, tools, features, and services, and how they interact with each other. 

You're not going to be expected to be an expert on everything, but part of the base understanding is comprehending the interactions. 

Let's get started with the second task statement and dive deeper into the AWS global infrastructure.


Define the AWS global infrastructure
–
Let's get started with the second task statement to define the AWS global infrastructure. Understanding the concepts behind the AWS global infrastructure will help you know how to connect, design, build, and deploy your architectures in AWS. 

Remember, we mentioned the AWS global infrastructure and its components such as Availability Zones, Regions, and edge locations in an earlier lesson. 

For this task statement you need to ensure you understand how they function, both individually and together, what choices do you have, and how do you determine when services use a specific Availability Zone, Region, or edge location? 

Let's dive a bit deeper than we dove in our earlier lesson. AWS offers globally resilient services, regional resilient services, and zonal resilient service. A globally resilient service is a service that operates globally, for example, a single database, and that data is then replicated across AWS Regions. So a Region can fail, but the service will continue to run. IAM, CloudFront, and Amazon Route 53 are examples of globally resilient services. 

Region Resilient services operate in one Region, with one set of that data in that Region, and then replicate their data to multiple Availability Zones inside that Region. If you lose one Availability Zone in that Region, the service will continue to operate, but if the whole Region fails, then so does that service. Amazon EFS and AWS Batch are examples of regionally resilient services. 

Availability Zone resilient services are run in a single Availability Zone, so if that Availability Zone fails, then so does that service. So an important exam tip is to know what level of resilience an AWS service has because it will help you answer exam questions, but also give you a much deeper understanding. Amazon EBS is an example of an Availability Zone service. 

There are also zonal services, such as Amazon EC2, that are tied to a particular Availability Zone. So what is the AWS global infrastructure? Well, AWS created their global infrastructure as a collection of individual infrastructures located all over the world. So globally, AWS offers Regions and also edge locations, and AWS is designed to have multiple Regions, multiple Availability Zones, and multiple edge locations around the world. 

The AWS Global infrastructure is made up of AWS Regions around the world, and then on top of the infrastructure, we have a high level of services. So you will see there is a level of compute, and examples of compute are Amazon EC2 and AWS Lambda. There's a level of storage and examples of AWS storage services are Amazon S3 or Amazon EFS. There's a level of database services such as Amazon RDS, Amazon Aurora, Amazon DynamoDB. There's a level of migration and transfer, network and content delivery, developer tools, robotics, blockchain, satellite, management and governance, machine learning, security, identity and compliance, application integration, cost management, and more. 

And an AWS Region is a geographical area that consists of two or more Availability Zones, and all of the Regions are under that Region's rules. If your data sits in a particular Region, your data will not leave that Region unless you specifically move that data. AWS designs their Regions to be geographically spread across the world. This way you can use different Regions to design your infrastructure that can withstand global disasters. So there is separation between Regions so each Region is fault tolerant. 



Here's an exam tip and one for the real world too. When you are selecting Regions for your applications in AWS, remember to check if there are compliance requirements and also choose the closest Region to reduce latency for your users. 



Regions also have Local Zones which are an extension of an AWS Region in closed geographic proximity to your users. AWS Wavelength is similar. It helps you to build applications that deliver ultra-low latency to mobile devices and end users. Wavelength has a wavelength zone which is an isolated zone in the carrier location where the Wavelength infrastructure is deployed. Wavelength zones are tied to an AWS Region and are extensions like the Local Zones. 

An Availability Zone is one or more data centers with redundant power, networking connectivity and so on. And the data centers are in separate facilities inside different AWS Regions. AWS has multiple data centers across the world. You can think of an Availability Zone as one or more data centers. And a data center is a building that's filled with servers, sands, switches, load balancers, firewalls, storage and more. You can have more than one data center in each Availability Zone, and each Availability Zone is an isolated compute storage network, and so on. You can distribute your infrastructure across multiple Availability Zones in your Region for high availability. So if one Availability Zone fails, then your other Availability Zones should remain operational because they are isolated from each other, but they are connected with high speed redundant networking. 

Your services can be placed across multiple Availability Zones to add resilience and high availability. And an AWS edge location, again, is a global service, and it is an endpoint for AWS that is used for caching content. AWS has a content delivery network or CDN, which is CloudFront. And CloudFront offers, that if a user requests certain information, that information is then cached at the edge location, so the next time another user requests that same information, that information is already available and delivered to the user much faster than if you had to go all the way back to the database and search for that specific information. 

An example of this is how Netflix uses edge locations to store content as close to their customers as possible. So customers get very low latency when requesting certain movies and shows. The further the data that is being requested is located from your customers, the slower the transfer of that data becomes. 

Another service of focus here is AWS Global Accelerator, which is a global service that supports endpoints in multiple AWS Regions to improve the performance of your applications for local and global users. For your exam, know the difference between CloudFront and Global Accelerator, because they both use the AWS Global Network and edge locations. Both also integrate with AWS Shield for DDoS protection. 

However, CloudFront will improve content for cache content, both static and dynamic content, and content is then served from that edge location most of the time. Global Accelerator improves the performance for applications over TCP and UDP because the packets are being proxied from the edge locations to the applications running in one or more Regions. So all the requests that are making it to the edge, but there is no caching available. Global Accelerator is great for HTTP use cases that need a static IP or need fast regional failover. 

Make sure you understand the differences between a Region, Availability Zone, and edge locations, because you will most likely see questions on these on your exam. Also ensure you know the models for cloud. For example, cloud computing has infrastructure as a service, platform as a service, and software as a service, but there is also database as a service and more. 

Next, look at the components separately. With Availability Zones, study which services are bound by their borders, and how the Availability Zones can help you when building a highly available environment. 

Also, look at how the Availability Zones come into play when you're determining points of failure in your architectures. Additionally, how is communication handled when going from one Availability Zone to another? This is gonna differ based on the services, so you'll need to look at which services deal specifically with the Availability Zones, and then how they connect and communicate across those Availability Zones. 

When considering Regions, you wanna look into a lot of the same stuff as you will with Availability Zones, but on top of that, you should look into ways to use Regions for disaster recovery and business continuity. What can you do to avoid catastrophic failures and what services help you utilize your use of the Regions? 

Also, understand why you would use the different Regions and what benefits you might find from the use of a particular Region or the use of multiple Regions. Lastly, study how Region usage of affects your compliance requirements. Oftentimes, data management is determined by sovereign borders. It is important that you know how such laws and requirements will affect your usage of the AWS Cloud. 

The final category for this task statement and for the global infrastructure you need to be mindful of is edge locations. Look into what services take advantage of these edge locations and how those services differ in their utilization. Can you choose which edge location to use? How are edge locations utilized by CloudFront? What about for AWS Global Accelerator? Do the edge locations come into play only with certain services, or are there other times when you're using them? What benefits are provided by the edge locations, and what level of management can you dictate? Availability Zones, Regions, and edge locations are foundational components to understanding and utilizing AWS. 

Understand how they help and affect your architectures, what you can do to take advantage of their benefits, and the services that are bound or directly interact with these components. 

Let's get started with the third task statement and talk about AWS compute services.


Identify AWS compute resources
–
Let's get started with the third task statement covering AWS compute services. And we will start with Amazon EC2. 

Amazon EC2 is virtualization as a service. It is an infrastructure as a service or IaaS product, and it provides so much value to your AWS accounts. 

Let's pause here and check your fundamentals. What is virtualization? Well, it is running one or more operating systems on a piece of physical hardware known as a server. Each operating system is separate, along with their applications, and also allows multiple different privileged applications to run on that same hardware using software to make their calls. And this virtualization started off with two different designs and then has kept evolving. If you're interested in the history of virtualization, please let me know. I have some content covering virtualization fundamentals. 

Let's start with the architecture for Amazon EC2. Amazon EC2 is the default AWS compute service. Amazon EC2 instances are virtual machines and EC2 instances run on EC2 hosts which are the physical hardware or physical servers that AWS manages. 

And the EC2 hosts are either shared or dedicated hosts. Shared hosts are the default for EC2 hosts that are shared among different AWS customers, and the customers do not get any ownership of the host hardware. You pay for your individual instances and resources, but it is important to remember that when you use a shared host, your instance is isolated from other AWS customers. 

With Dedicated Hosts, you are paying for the entire EC2 host, not just the instances that you run on that host. You do not share it, you pay for the entire host, no matter how many instances you spin up. EC2 instances are an Availability Zone resilient service, because the EC2 hosts sits in an Availability Zone. So if that Availability Zone fails, then the host and the instances running on the host will also fail. Now, EC2 hosts that sit inside an Availability Zone also have local storage called instance store. 

And the key to understand with instance store is that it is temporary storage. And if your EC2 instance moves off the host to another host, then your storage in the instance store is lost. And for networking, when an EC2 instance is launched into a specific subnet inside your Amazon VPC, a primarily Elastic Network Interface is provisioned into that subnet and it is mapped to the physical hardware of that EC2 host for that Availability Zone. 

And you can add multiple different Elastic Network Interfaces to your EC2 instances. 

So let's go back to storage and talk about how EC2 instances can connect to an Elastic Block Store or Amazon EBS. Amazon EBS lets you access volumes of persistent storage. Inside your Amazon VPC, you have a data network set up for your Elastic Network Interfaces, but you also have a storage network to connect to your EBS volumes. 

Amazon EBS is an Availability Zone resilient service, so you can have different EBS volumes running in different subnets for different EC2 instances, but you cannot connect to EC2 instances to an EBS volume in a different Availability Zone or subnet. 

For the exam, know that there are different types of EC2 instances and a high-level overview of which instance type would be the best choice for a particular scenario. When you choose and launch an Amazon EC2 instance, you get a raw amount of CPU, memory, local storage, and type of storage. And be aware of the performance for each instance type because with each instance type, you also get storage and network bandwidth. And you need to make sure you have enough bandwidth with the instance type you choose. 

But we also get resource rations when we choose a particular EC2 instance. And resource rations are simply the amount of resource you get for each raw amount. For example, an instance that is more suitable to compute would give you more CPU and less memory than an instance more suited for memory which would give you more memory and less compute. You also get additional features and capabilities with different instance types like GPU for graphic processing. 

And a key here and another fundamental to know, your AWS account structure, your goals, and your design so you know which instance type to choose. Amazon EC2 instances are group into five instance categories, and these categories help you select an instance type based on your particular workload. 

General Purpose is great for default steady state workloads, even resource rations, and should be used as default unless you have specific requirements. 

Compute Optimized instances are designed for high-performance computing such as media processing, machine learning, gaming, scientific modeling, and so on. The resource rations are usually more CPU than memory and they provide access to higher performance CPUs. 

Memory Optimized is great for processing large in memory datasets and database workloads. The resource rations are usually more memory than CPU. 

Accelerated Computing is designed for specific requirements such as hardware GPU and field programmable gate arrays. 

Storage Optimized is designed for applications using data warehousing, analytic workloads, Elasticsearch, sequel and random I/Os. 

And then there are EC2 instance types for each of these instance categories. We also have burstable instances. And what happens here is that instances have normal CPU loads that are low and you are given an allocation of burst credits that allows you to burst up and then return to the normal level. These are usually cheaper and a great option. 

For the exam, know that you can create custom Amazon Machine Images, or AMIs, for your Amazon EC2 instances. A golden AMI is an AMI that contains the latest security patches, software, configuration, and software agents that you need to install for logging, security, maintenance, and performance monitoring. 

Let's dive a bit deeper. Do you know where to find the Amazon EC2 instance ID, instance profile permissions, and kernel information? Would you check the instance metadata, instance user data, or AMI? Instance metadata is the data about your instance you can use to configure or manage. You can get the instance ID, public key, public IP address, and other information from the instance metadata using http://169.254.169.254/latest/metadata using http://169.254.169.254/latest/meta-data using http://169.254.169.254/latest/meta-data Instance user data is incorrect because this is used to perform common automated configuration tasks and run custom scripts after the instance starts. It doesn't contain any information about the instance ID, public keys, or the public IP address of your EC2 instance Another type of compute is containers. 

And for the exam, we need to understand what containers are and what benefits container computing provides. The big difference between Amazon EC2 and containers is that the guest operating system on Amazon EC2 instances takes up a lot of space, and those resources are consuming memory and disk space, and this leaves just a little bit of space for the applications. And a lot of these instances are using the same operating system. So there is a lot of resources being consumed, lots of usage, and lots of duplication. 

Containers are designed to handle it all differently. You have the hardware and you only have one operating system on top of the hardware. Then on top of the operating system is a container engine, and the container runs as a process within the host operating system. So they can use the host operating system for networking and more. Amazon Elastic Container Service, or Amazon ECS, is a service that accepts containers along with instructions on where and how to run those containers. It is an AWS managed container-based compute service that is a container orchestration service. 

There's also Amazon Kubernetes Service or Amazon EKS that allows you to run AWS powered Kubernetes on Amazon EC2 instances. 

Another AWS compute service is AWS Lambda which is a function as a service or FaaS product. Lambda accepts functions which are a small piece of code written in a language. They use a runtime, like Python, Java, node.js, and you are billed for the duration of your execution. So it is an event-driven service and Lambda functions are invoked on an event occurring. You develop code in a language. You pick a runtime using that language, and then you can execute that code based on the triggering of an event. 

Lambda is considered to be serverless and is a great compute alternative for a serverless architecture. Whether you're working with functions in AWS Lambda, application stacks in Amazon Elastic Beanstalk, container management in Amazon ECS, or keeping it traditional with EC2, there are many compute options within AWS. 

We have talked about the importance of high availability and scalability and auto scaling and load balancers are used to achieve high availability, fault tolerance, and elasticity. Auto scaling groups are how we configure Amazon EC2 to scale automatically based on different criteria. Load balancing is a method used to distribute incoming connections across a group of servers or services. And incoming connections are made to the load balancer, which then distributes them to the servers or services. 

In AWS, we have four choices for load balancers: Classic Load Balancer, Application Load Balancer, Network Load Balancer, and the Gateway Load Balancer. And these four make up a family of Elastic Load Balancers and they are great to pair with an auto scaling groups to enhance the high availability, fault tolerance, and scalability of an application. 

When you create a load balancer, you are creating one entity, one load balancer, but it actually creates an ELB node. So for high availability, a load balancer node should be placed in each Availability Zone that a load balancer uses. 

For the exam, know the appropriate use of different EC2 instance types, appropriate use of different container options, and the appropriate use of different serverless compute options. Also, understand that auto scaling provides elasticity and how to use both auto scaling and load balancing together. 

Let's get started with the fourth task statement and we'll talk about AWS database services.


Identify AWS database resources
–
Let's get started with the fourth task statement covering AWS database services. I want to again mention fundamentals for this exam. You should understand what a database is and how it works. 

Let's start with Amazon RDS, which is a Database as a Service. Amazon RDS provides managed database instances. AWS handles the management, and you get access to a managed database instance that provides lots of benefits and performance enhancements. We do not have to manage the physical hardware, the operating system and more. RDS supports MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server and Amazon Aurora. 

You can pick which database engine works with your application requirements. And Amazon RDS databases comes in different types, sizes, and families, just like Amazon EC2 instances. RDS instances can be in one Availability Zone or in multiple Availability Zones. For an RDS instance, you allocate storage for that instance to use, so it is dedicated storage for that database instance. And access to and for your database instances are controlled through security groups. Amazon RDS instances in a single Availability Zone can be vulnerable to a failure in that single Availability Zone because the instance and the storage are in a single Availability Zone. 

But you can choose a multi-AZ configuration that will allocate secondary hardware in a different Availability Zone. This second database instance is called the standby replica instance. RDS uses synchronous replication from the primary and the standby instance. You can also configure an RDS read replica that provides two main benefits, performance benefits and availability benefits. 

A read replica is a read-only replica of your RDS database instance, and read replica are for read operations, and that data is replicated asynchronously. Synchronous replication is completed in multi-AZs, and the data is written to the primary instance and replicated to the standby read replica at that same time. 

On the exam, if you see asynchronous, think read replicas, and synchronous replication would be the choice for any question with a multi-AZ environment or scenario. 

Another relational database service is Amazon Aurora, but it does have quite a few differences and improvements over Amazon RDS. First, the Aurora architecture is very different from Amazon RDS. Amazon Aurora uses the base architecture of a cluster, and it is compatible with MySQL and PostgreSQL engines. It is a cluster made up of a single primary instance and then zero or more read replicas. This may seem the same as Amazon RDS, but Aurora read replicas can provide the benefits of reads and also the benefits of multi-AZ. They can be used to improve availability and read operations on your cluster. 

Second, Aurora storage is different from RDS because Aurora does not use the local storage for the compute. It actually uses a shared cluster volume, which provides faster provisioning, improved availability, and better performance. Aurora Serverless is a service of Aurora and provides a version of the Amazon Aurora database product where you do not have to provision or manage the database instances. Aurora Serverless works a bit differently architecturally too. 

Aurora Serverless provides the same shared cluster storage, so you get, again, six copies of your data across three Availability Zones. But you do not provision your cluster the same way. For Aurora Serverless, you use an ACUs, which is Aurora capacity units. The Aurora ACUs provides a certain amount of compute and a corresponding amount of memory. For an Aurora Serverless cluster, you can choose a minimum ACU and a maximum ACU, and then your Aurora Serverless cluster will scale between that minimum and maximum value and will add and remove the capacity in your cluster based on your load. A really cool feature of Aurora Serverless is that the ACU can even go down to zero and then can even be paused after a period of inactivity. This is a great cost savings because when your cluster is paused, you're only paying for the storage that you're using. 

Aurora global databases are a feature of Aurora provisioned clusters which allow data to be replicated globally, providing significant recovery point objective and recovery time objective improvements for business continuity and disaster recovery planning. Additionally, global databases can provide performance improvements for customers with that data being located closer to them and in a read-only form. Replication occurs at the storage layer and is generally less than 1 second between all AWS Regions. 

Let's move on and talk about the non-relational databases that AWS offers. Do you know which AWS service that is? Amazon DynamoDB is a NoSQL Database as a Service product, and it is a public service and sits in the AWS Public Zone. AWS manages DynamoDB for you. You do not need to manage any servers or infrastructure, and DynamoDB can handle simple key-value data or structure data. DynamoDB also supports scaling options using provisioned capacity with manual controls or use an on-demand mode which handles that scaling for you. 

DynamoDB is highly resilient across multiple Availability Zones in a Region, or you can configure a global table to add global resilience, but that will be an extra cost. 



Let's also cover in-memory-based databases. Up first is Amazon ElastiCache, which is a managed in-memory cache for performance improvements for reads. It supports two different popular caching engines, Redis and memcacheD. So if you see a question with memcacheD or Redis, your answer is probably ElastiCache. It is designed to store reads and deliver those results from in-memory caching and improves performance. So instead of consulting the database each time for reads, you can cache results and then deliver those results quicker. ElastiCache can also be used to store user session states. And we mentioned sessions in the first domain under scaling and high availability. 

But you can store the user's session states using ElastiCache, and it can be used as a performance-enhancing tool or used with fault-tolerant architectures. 

Let's also talk about Amazon DynamoDB Accelerator or DAX because ElastiCache can also be used with DynamoDB, but AWS created DAX to be used specifically with DynamoDB. Amazon DynamoDB is designed to be a low-maintenance and high-performing database that provides access to your data in milliseconds. But there are edge locations that need this data faster, like microseconds, and also for high-read sessions, and DAX provides an in-memory cache for DynamoDB. 

Here's a quick exam tip. DAX is designed for latency-sensitive and high-read workload. You may see a question asking what is the best caching product to use with DynamoDB? And the answer is DAX. DAX is not great for applications that need strongly consistent reads. DAX is more for eventually consistent reads. 

Before we move on and talk about database migration tools, let's also cover Amazon Redshift. Redshift is a petabyte-scale data warehousing solution from AWS. It is a column-based database engine for analytic workloads. Most RDS databases are used for OLTP, which is online transaction processing. But Redshift is designed for OLAP, which is online analytical processing, and it is not the type of database where you update individual records. Redshift is used for data warehousing analytics, and everything is stored in columns. And column-based databases are great at queries because all that data is in the columns. 

Row-based databases are great for transaction-type processing, and column-based databases are great for analytics. Redshift is based on PostgreSQL, but it is an online analytical processing. Redshift is not used for online transactional processing. Redshift uses a cluster architecture and unloads and uploads data to S3. It can also accept data from DynamoDB, the Database Migration Service and other databases, and Amazon Kinesis too. 

And to go along with Amazon S3, we can use Redshift Spectrum to perform queries directly against S3. Here's another exam tip with keywords. Know that Redshift is a data warehouse used for analytical and summarization transactions and can also scale to almost any workload needed, and it is a petabyte-scale database. 

For the exam, you should also know what AWS database migration tools are available to help you migrate your applications to AWS. AWS offers the AWS Snow Family of services to migrate large amounts of data in and out of AWS. The AWS Snowcone, Snowball, and Snowmobile are a collection of physical devices that help you migrate large amounts of data into and out of the cloud without depending on networks. This helps you apply the wide variety of AWS services for analytics, file systems, and archives to your data. 

Here is a question. Which AWS Snow Family service would you use to move terabytes to petabytes of data into AWS using appliances that have storage and compute capabilities? AWS Snowball Edge is a data migration and edge computing device that comes in two options, Snowball Edge Storage Optimized and Snowball Edge Compute Optimized. 

AWS Snowcone is only for 8 terabytes, and AWS Snowmobile is for petabytes to exabytes of data. Snowball Edge supports specific Amazon EC2 instance types as well as AWS Lambda functions, so customers may develop and test in AWS and then deploy applications on devices in remote locations to collect, pre-process, and return that data. Common use cases include data migration, data transport, image collation, IoT sensor stream capture, and machine learning. 

Another migration service is the AWS Database Migration Service, or DMS, which is a managed service capable of both data migration and schema conversion. Traditionally, a database migration is done by either a backup and restore. You have to stop all input and output operations on your database, take the backup and then do a restore, and then update your applications to the new database endpoint. And this will require a full outage during your migration. You can use replication and configure the replication instance between the source and target database. Then allow replication to replicate all existing data, and then migrate any new transactions too. And this usually has very little downtime. You simply update the application to point to the new endpoint. But this replication process is very admin intensive, and it is difficult to set up right. 

So AWS offers DMS that handles this overhead and configuration for you. You create a replication instance and define a source endpoint and a target endpoint with any authentication that may be required. And all that data is migrated from the source to the target, and operations continue on that source database until you decide to migrate it all to the target. 

AWS also provides the AWS Schema Conversion Tool to help transform between different database engines for your migration. DMS is great for scaling your resources up or down with little downtime, or migrating databases from on premises or from AWS to on premises, and also other cloud platforms. And the Schema Conversion Tool allows us to move data between different database engines. 

Before we wrap up, let's also talk about AWS DataSync. AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server. Manual tasks related to data transfers can slow down migrations and burden your IT operations. 

DataSync automatically handles many of these tasks, including scripting copy jobs, scheduling and monitoring transfers, validating data and optimizing network utilization, and can transfer hundreds of terabytes and millions of files at speeds up to 10 times faster than open-source tools, over the internet, or AWS Direct Connect links. You can use DataSync to migrate active datasets or archives to AWS, transfer data to the cloud for timely analysis and processing, or replicate data to AWS for business continuity. 

With the database services, a major area of focus will be being able to differentiate between which services fit specific scenarios. This involves both comparing services like Amazon RDS, Amazon DynamoDB, and Amazon Redshift to each other, as well as comparing them to managing your own database on EC2 instances. 

Here are some questions to consider. What advantages do you get from utilizing the managed service? How does the managed services differ from that of a database engine installed on an instance when it comes to customability, management, and performance? What are the limitations of each style of database? Understand how these services will fit into your own builds, or hypothetical builds, and what role each could play. There is no one-size-fits-all solution. 

The managed services within AWS are varied, so when considering them, it is important to look at not only the variety of managed services that exist, but also at the varying levels of management they provide. 

In the same way that Amazon DynamoDB and Amazon RDS vary in how they are managed, it is also different when looking at Amazon EKS, AWS Fargate, and so many others. Evaluate where the benefits lie, how you can use them in conjunction with each other in an architecture, what they complement or replace, and how topics like availability, failover, and data consistency come into play. 

The features and customization options of each database service will be very useful in matching your requirements in different scenarios and use cases. 

Let's get started with the fifth task statement and talk all about AWS network services.


Identify AWS network resources
–
Let's get started with the fifth task statement and talk about AWS network services and controlling and optimizing access and communication. Again, before we jump into this lesson in the AWS networking services, I always recommend fundamentals, especially for networking fundamentals. 

Everything you do in AWS uses networking. When you communicate with AWS, when you create connections between your on-premises environment in AWS, and so on. This is why a solid network design is the foundation of any environment. However, designing and configuring your foundation for networking is different in AWS than it is for on-premises. 

Amazon Virtual Private Cloud, or Amazon VPC, helps you to control access to your resources. It is a virtual private cloud, so basically it is your very own data center in the cloud. You can provision your own Amazon VPC in a logically isolated section of AWS and launch different resources and create your own Virtual Private Network. You have complete control over this virtual networking environment, and this even includes choosing your own IP address ranges, creating your own subnets, and configuring access with your route tables and network gateways. 

An Amazon VPC offers multiple different layers of security, network access control lists, and security groups that you can configure to control who and what is allowed to access resources inside your Amazon VPC. So again, an Amazon VPC is a virtual private network, and it is a service that we use to create private networks in AWS, and our private services will run from this Amazon VPC. 

You can also connect your Amazon VPCs from your AWS private network to your on premises for a hybrid environment, or you can connect to other cloud platforms when you're creating a multi-cloud environment. When you create an Amazon VPC, it is in one Region and in one AWS account, so this makes an Amazon VPC a regional service, and VPCs operate resiliently by operating in multiple Availability Zones in a specific Region. 

By default, your Amazon VPC is isolated and private until you explicitly grant public access, but there is one exception, and you need to know this for your exam, and that is the default Amazon VPC. There are two types of Amazon VPCs in AWS, the default VPC, and you can have only one default Amazon VPC per Region, and also custom Amazon VPCs. 

Custom Amazon VPCs are configured by you and you are responsible for all of the configurations, and by default they are isolated and private. 

Default VPCs are created by AWS when you create your AWS account, and AWS sets up the configuration for you. Each default VPC comes with one VPC CIDR range, which is a given range of IP addresses, and this VPC CIDR defines the start and end range of the IP addresses that this default Amazon VPC can use. So, everything inside your Amazon VPC uses the CIDR range. All communications to the Amazon VPC will need to use the VPC CIDR, and outgoing communications will be from this VPC CIDR. 

All default Amazon VPCs are configured in the same way, and with your Amazon VPC, you can divide your network across the Availability Zones for resilience so you can subdivide your Amazon VPC into subnets, and subnets is short for sub-network. Each default Amazon VPC is configured to have one subnet located in each Availability Zone of that Region, and each subnet in your default Amazon VPC uses part of the IP address range of the VPC CIDR. 

If one of your Availability Zones fails, then that subnet in that failed Availability Zone will also fail. But with the default Amazon VPC, you have other subnets in other Availability Zones that are still operating. 

So ensure you know how an Amazon VPC can help you to control access to your resources. First, you should understand that every Amazon VPC has a VPC router that is highly available, and it moves traffic from somewhere to somewhere else, and it runs in all of your Availability Zones that your Amazon VPC uses. This VPC router has a network interface in each subnet in your Amazon VPC and uses the Network+1 address. Again, this is part of the fundamentals of networking you should understand at a high level. You never need to worry about this VPC router. It just works and it is managed by AWS and it routes traffic between subnets in your Amazon VPC. 

You can control this router a bit by creating route tables and associate the route table with the subnet and then add rules to allow traffic in and out of your subnets. Each Amazon VPC has a main route table associated with your subnets, so if you do not explicitly associate your new route table with your subnet, then the Amazon VPC will use the main route table, and a subnet can only have one route table associated with it at a time, but you can use one route table for many different subnets in your Amazon VPC. 

Let's also talk about the internet gateway. Your Amazon VPC can only have one internet gateway at a time, and for your default Amazon VPC, the internet gateway is already attached. The internet gateway is a regional resilient service. It is highly available and it sits on the edge of your Amazon VPC and the AWS public zone and the internet, and manages traffic between your Amazon VPC, the AWS public zone, and the internet, and the reason that we only have one public route table is that the internet gateway is per Amazon VPC. 

So one internet gateway works across all Availability Zones. If you create a new internet gateway, you must attach it to the Amazon VPC. It is not automatically attached. Then you need to add a route to your route table to allow traffic to and from the internet gateway. And if we add an internet gateway route to a subnet, then that becomes a public subnet because it now has access to the AWS public zone and the open internet. 

Another way to control access is to use network access control lists, which is a type of security filter like a firewall which can filter traffic as it enters and leaves a subnet. Network ACLs are attached at the subnet level, and by default a default network ACL is created for your default Amazon VPC and is associated with all the subnets by default. Remember, network ACLs are used for traffic entering or leaving a subnet because network ACLs are associated with the subnet and not the resources. Network ACLs only manage traffic that is crossing the subnet boundary. Back to fundamentals. Network ACLs are stateless. We covered this under domain 2, but remember, stateless means if you add a rule for inbound traffic, then you must also add the same rule for outbound traffic. Network ACLs only see the traffic going one way. So if you allow an inbound rule, you must also allow an outbound rule so your network ACL will explicitly see that that traffic that was allowed inbound is also allowed out. Network ACLs see the traffic as two separate streams. So you must have two rules, one rule for each stream. 

Security groups are another security feature of AWS, only, unlike network ACLs, they are not attached to the subnet. Security groups are attached to the elastic network interface of the AWS resources in the subnet. They also work differently from network ACLs. Security groups sit at the boundary per se of the instance instead of the subnet. Security groups also have inbound and outbound rules, but security groups are stateful. Remember, stateful means that if traffic is allowed in, then that traffic is automatically allowed back out. Security groups see both the inbound and outbound traffic as part of the same stream. 

Let's also cover another fundamental and talk about Network Address Translation, also known as NAT. NAT is the process of giving a private resource outgoing access to the internet. An example is the internet gateway. The internet gateway performs a type of NAT called static NAT. The internet gateway allocates a resource with a public IPv4 IP address so when the data or packets leave that resource and pass through the internet gateway, the internet gateway switches the source IP address from the private IP address to the public IP address and then sends that packet on. When the packet returns, it switches the destination address from the public IP address back to the private IP address. So NAT gives a private CIDR range outgoing internet access and also to the AWS public zone. And when private resources initiate traffic with the NAT gateway, they can receive responses back in. But outside traffic from the internet cannot initiate traffic inbound. 

So you can use NAT gateway to allow private resources access to the internet or AWS public zones. Well, why might a private service need access to the internet? Well, the most common reason is for software updates. 

Let's move on and talk about communication in your Amazon VPC. VPC peering is a way to link multiple Amazon VPCs together and allows direct communications between two isolated sites using their private IP addresses. VPC peers can span AWS accounts and also Regions, and the data shared is encrypted using the AWS global infrastructure. VPC endpoints are gateway objects we create inside your Amazon VPC, sort of like an internet gateway and NAT gateways to allow instances inside and Amazon VPC to connect with AWS public services without the need of a gateway. 

There are two types of endpoints that we need to know for the certification exam, gateway endpoints and interface endpoints. A gateway endpoint is used for AWS public services. Remember, some AWS services are public services and they sit inside the AWS public zone. And sometimes we want to connect to these public services like S3 or DynamoDB from a private instance or subnet that does not have access to the internet and does not have a NAT gateway set up. 

Interface endpoints are used for everything else, and you have to pick the correct endpoint depending on the AWS service. Interface endpoints use DNS, not routing with a prefix list, and are for all other services besides S3 and DynamoDB. 

Let's also mention AWS PrivateLink, which is a VPC endpoint service. PrivateLink solves the problems of needing to expose an application to other Amazon VPCs in other AWS accounts, and it does not require VPC peering or other gateways. 

You can also create your own hardware virtual private network or VPN connection between your on-premises data center and your Amazon VPC and use AWS as an extension of your current data center or current environment. Remember from an earlier lesson this is called a hybrid environment. 



AWS has a service called AWS VPN, and it is a service where you can configure a hardware VPN, which is a highly available virtual private connection between your Amazon VPC and on-premises networks. VPNs use the public internet as a transit route for your on-premises and Amazon VPC. VPNs offer a fully encrypted route from wherever your on-premises network is located to your Amazon VPC. 



We also have another option to connect your on-premises networks and your Amazon VPC, and that is AWS Direct Connect. Direct Connect is a dedicated physical connection between your on-premises network and AWS. For the exam, remember that dedicated is a keyword for Direct Connect. A VPN connection is a virtual private connection over the public internet. Direct Connect is a physical piece of fiber running between your on-premises network and AWS's network. It is a physical or cross-connect connection between an AWS router and another router. And AWS has a number of Direct Connect locations that are distributed globally. 

Another fundamental needed for this exam is DNS, Domain Name System. Amazon Route 53 is AWS's managed DNS product, and it essentially helps with two things. First, you can register domains, and second, Route 53 can host zones on managed nameservers. Amazon Route 53 is a global service with one single database, and it is replicated between Regions, making it a globally resilient service. This means that it can tolerate the failure of one or more Regions and continue to operate. 

DNS is a service used to help discover other services on the internet, and it translates addresses from the language that computers understand to the language that humans understand and then back again. So if we wanna go to amazon.com, we type in amazon.com, but computers use IP addresses. So DNS translates our words into IP addresses and then finds that location. It then sends that location back to us and translates IP address back to the words that we can understand. 

Amazon Route 53 has routing policies that can help ensure high availability and resilience too, such as failover, weighted, and latency routing policies. 

And then of course, this task statement also covers edge services such as CloudFront and Global Accelerator, which we've already talked about. 

For the exam, ensure you know how to take advantage of the features, controls, and configurations to build privacy and security into your environments. Know what you can do to provide connectivity to your Amazon VPCs from the internet or from your on-premises location, and when to use VPN, Direct Connect, and Amazon Route 53 for different use cases. 

Let's get started with the sixth task statement and talk about AWS storage services.


Identify AWS storage resources
–
Let's get started with the sixth task statement covering AWS storage services. Again, a fundamental needed is to understand what is cloud storage. 

Cloud storage is a cloud computing model that stores data through a cloud computing provider that manages and operates data storage as a service, giving you the agility, global scale and durability with any time, anywhere data access. 

So how does cloud storage work? Cloud storage is purchased from a third-party cloud vendor who owns and operates data storage capacity and delivers it over the internet in a pay-as-you go model. These cloud storage vendors manage capacity, security, and durability to make data accessible to your applications all around the world. 

There are benefits of cloud storage, such as no hardware or storage to provision, on-demand capacity, performance and retention, storage lifecycle management to move data to lower cost tiers and only pay for the storage you need and use. And there are cloud storage requirements such as durability, availability, and security to ensure your data is safe, secure, and available when needed. 

There are types of cloud storage you need to know for the exam. Object storage, file storage, and block storage. Each offers their own advantages and have their own use cases. 

Let's start with object storage. Object storage solutions such as Amazon S3 is a global resilient service, so it is a global storage platform. Amazon S3 is run in every AWS Region and your data is also stored in a specific Region and can be accessed from anywhere. And remember, Amazon S3 is a public service and runs from the AWS public zone. Your data stored in S3 is replicated across different Availability Zones in that Region. S3 can tolerate a failure of an Availability Zone and can also replicate data between Regions, too. It is designed to store virtually an unlimited amount of data and multiple users can access your data in S3. And keywords for S3 are unlimited amount of data. With Amazon S3, you can create buckets and add objects to your buckets. The S3 buckets hold your objects like a container. Objects can be videos, pictures, files, large datasets, and more. And the size of the object in S3 can be from 0 bytes to 5 terabytes in size. Let's mention compliance, too, because when you store objects in an S3 bucket, that S3 bucket is in a specific Region and the laws and rules of that Region also apply to your S3 bucket. 

When you create an S3 bucket, you must add a globally unique name for that bucket. No one bucket can have the same name as another bucket, no matter which Region or where in the world that S3 bucket sits. Here's a question. What feature of Amazon S3 can you use to ensure the objects stored in your S3 bucket are not accidentally overwritten or deleted? S3 has a lot of great features to organize and manage your data for different use cases to enforce security and meet compliance requirements. S3 features can append metadata tags to objects, move and store data across the S3 storage classes, configure and enforce data access controls, secure data against unauthorized users, run big data analytics, monitor data at the object and bucket levels, and view storage usage and activity trends across your organizations. Objects can be accessed through S3 access points or directly through the bucket host name. But to answer the question, if you enable S3 versioning during the bucket creation, every time you upload an object, it will automatically create a version of it. So if you accidentally overwrite an object, you can still use versioning to restore the previous object. 

For your exam, take some time to understand the different S3 storage classes and use cases. If you are asked to choose the most cost-effective storage class to store archives for a long time and need a retrieval time of less than 12 hours, which S3 storage class do you choose? Amazon S3 Glacier or Amazon S3 Glacier Deep Archive? Glacier Deep Archive is the lowest cost storage class and supports long-term retention and data can be restored within 12 hours. If you are asked to choose an object store system, then that choice should be Amazon S3, because S3 is an object store system. It is not a file system like Amazon EFS or a block storage system like Amazon EBS. 

Let's check out those too. File storage is another storage option in AWS. Some applications need to access shared files and require a file system. This type of storage is often supported with a Network Attached Storage server. File storage solutions such as Amazon EFS are ideal for use cases like large content repositories, development environments, media stores, or user home directories. Amazon EFS is useful with most AWS infrastructures, because it provides network-based file systems that can be mounted with Linux EC2 instances and then can be used by multiple instances at the same time. The data is stored outside of the Amazon EC2 instance, but sits inside your Amazon VPC, so it provides scaling and self-healing properties. It can be accessed using hybrid methods like VPN, Direct Connect, and VPC Peering and you can also use lifecycle policies to move data between the two different storage classes in Amazon EFS. Amazon FSx is also a file server similar to Amazon EFS. Linux is a keyword for Amazon EFS. EFS is a shared file system for Linux and you cannot use Amazon EFS for Windows. 

So AWS created FSx for Windows and it is a fully managed Windows file system share drive that supports the server message block protocol and Windows network technology file system. It supports active directory integration and access control list. It is built on a solid state drive, or SSD, and is a highly scalable distributed file system for Windows managed by AWS. It can scale up to 10 gigabytes per second and millions of IOPS. It is highly available and can be configured for multi-AZ and can be accessed from your on premises, and your data is backed up daily to S3. There is also Amazon FSx for Lustre, and Lustre is a type of parallel distributed file system for large scale computing. Lustre's name comes from Linux and cluster, so that will help you remember it. It is for Linux instances and cluster is for large-scale computing. 

For the exam, know that Lustre is great for machine learning and high-performance computing and has a sub-millisecond latency. Lustre is also great for file systems to perform video processing, financial modeling, design automation, whatever needs a high level of distribution. 

And then there is also another option, which is block storage. Some enterprise applications such as databases often require dedicated low-latency storage for each host. This is comparable to direct-attached storage or Storage Area Network. Block-based cloud storage solutions such as Amazon EBS are provisioned with each virtual server and offered the ultra low latency required for high-performance workloads. In an earlier lesson, we mentioned that Amazon EC2 has local storage included with the AMI and this storage is called direct-attached or local-attached storage. These are physical disks that are directly connected to a device. 

This storage is directly connected to the Amazon EC2 host and it is called Instance Store and this type of storage is really fast, because it's attached to the hardware. But it does have a lot of issues. If the disk fails, if the hardware fails, if the instance moves between two different EC2 hosts, the storage can be lost. 

So we have an alternate type of storage we can use with our EC2 instances and that is Network Attached Storage and this is where volumes are created and attached to the EC2 instance over the network and AWS uses Amazon EBS for this. EBS is not as fast as Instance Store, but EBS is highly resilient and is separate from the instance hardware. So issues with the EC2 host will not impact EBS. You may also hear the term ephemeral storage and this means storage that is temporary. You cannot rely on this storage to be persistent. Instance Store is an example of ephemeral storage and EBS is an example of persistent storage. EBS is a volume that is presented to the operating system as a collection of blocks, but there is no structure, just a collection of addressable blocks And these blocks can be mounted, which means that file systems can be created on top of the block storage and you can also boot off of an EBS volume and this is why most EC2 instances use an EBS volume, block storage, as their boot volume and it is what stores the operating system. 

Here's a question. What type of EBS volume is recommended for most workloads and can also be used as a boot volume? Amazon EBS provides several volume types that differ in performance and price. The answer would be general purpose SSD, because it is recommended for most workloads. It can be used as a system boot volume and it is best for development and test environments. But there is also Provisioned IOPS SSD, which is meant for critical business applications that require sustained IOPS performance. It is best used for large database workloads. And there is Throughput Optimized HDD, which is meant for streaming workloads, requiring consistent, fast throughput at a low price. Big data, data warehouses, and log processing are great for this, but it cannot be a boot volume. And finally, Cold HDD, which is meant for throughput oriented storage for large volumes of data that are infrequently accessed or in scenarios where the lowest storage cost is important. It cannot be a boot volume too. 

You can also use EBS snapshots to create a point-in-time backup. Let's also talk about the cached file system for AWS, which is AWS Storage Gateway. AWS Storage Gateway is a service that allows us to connect our on-premises data center storage to an AWS storage service and it helps to migrate part or all of your storage platform to AWS or it supports extending your storage platform to AWS. Storage Gateway is a virtual appliance that you run on your on-premises virtualization platform, so you can run it on VMware, ESXi, EC2, and so on. 

So Storage Gateway is a product you download and configure on your on-premises location and it talks to AWS over the internet. There are three types of storage gateway that are available. Storage Gateway File Gateway that stores files as objects in S3, but has a local cache for your most recently accessed data on site. File gateway is a great option for Windows and is accessible using the SME protocol and when you upload data with your Storage Gateway, that data is stored in S3. Volume gateway is configured the same way as file gateway. So you configure and download the virtual machine image, but when you access it, you are not accessing file shares, but volumes and these volumes are accessed over the protocol iSCSI, which stands for Internet Small Computer Systems Interface and it is an internet protocol-based storage networking standard for linking data storage facilities. iSCSI provides block level access to storage devices by carrying SCSI commands over TCP/IP network and are generally used with SAN products. 

So it is Network Attached Storage and volume gateway works great with servers. And you have two options for volume gateway. Gateway Stored Volumes to store all your data on a volume gateway appliance itself and then snapshots are taken into AWS and stored on S3. It's great to use when you want your data on-premises, but snapshots and backups in S3. And then there is also Gateway Cached Volume to store primary copies of your data in AWS and then downloads and caches frequently accessed data on the volume gateway itself. 

The last type of storage gateway is the Virtual Tape Library Gateway to present a virtual tape library over iSCSI to any compatible backup software. This has a very high administrative overhead and is also costly and should be stored off site for disaster recovery and best practices. It allows us to present a virtual tape drive and it is stored in S3 and you can use this for your migration for your data into AWS over a period of time. 

Let's wrap up this lesson and talk about storage backups. Backup and recovery is critical to ensure your data is protected and accessible, but keeping up with increasing capacity requirements can be a constant challenge. Cloud storage brings low cost, high durability, and extreme scale to backup and recovery solutions. 

Amazon S3 offers a full range of storage classes, which offers trade-offs between price, durability, performance, and speed of access. Each storage class offers different cost, performance, retrieval speed, resilience, and so on, but it is a great choice and a low-cost storage you can use for backups. Embedded data management policies such as S3 Object Lifecycle Management can automatically migrate data to lower cost tiers based on frequency or time settings. 

And archive vaults such as Glacier and Glacier Deep Archive can be created to help comply with legal or regulatory requirements. These benefits allow for tremendous scale possibilities within industries such as financial services, healthcare and media that produce high volumes of data with long-term retention needs. 

There's also AWS Backup, which is a fully managed service that helps to centralize and automate data protection and meet compliance requirements across AWS services and on premises. Using this service, you can configure backup policies and monitor activity for your AWS resources in one place. It allows you to automate and consolidate backup tasks that were previously performed service by service and it removes the need to create custom scripts and manual processes. 

The AWS storage services vary for different use cases, functionality, optimizations, management, and more. Ensure you take some time to understand how each of these services work, not only how the services operate in their limitations, but also look into their actual usage. The AWS storage services vary in their use cases, functionality, optimization, management, and more and you should take some time to understand how each of these services work and how they communicate, how they might work with each other and of course, how they fit into the types of architectures you will build. 

Let's get started with the seventh task statement.


Identify AWS AI and ML and analytics services resources
–
Let's get started with the seventh task statement covering AWS artificial intelligence, machine learning, and analytics services. And again, let's pause for fundamentals. 

What is machine learning and artificial intelligence? Machine learning is the science of developing algorithms and statistical models that computer systems use to perform tasks without explicit instructions, relying on patterns and inference instead, to use machine learning algorithms to process large quantities of historical data and identify data patterns. This allows them to predict outcomes more accurately from a given input dataset. For example, data scientists could train a medical application to diagnose cancer from x-ray images by storing millions of scanned images and the corresponding diagnoses. 

Artificial intelligence is the field of computer science dedicated to solving cognitive problems commonly associated with human intelligence such as learning, problem solving, and pattern recognition. 

Let's dive a bit deeper. For your workloads in AWS, you can choose from three different levels of machine learning services. Artificial intelligence services, machine learning services, and machine learning frameworks and infrastructure services. For use cases such as anomaly detection, fraud detection, customer churn, and content personalization or recommendation engine. AWS provides a lot of services for this task statement, so take a deeper dive. 

You should know the different services and the tasks they accomplish. Let's start with the AI services level that provides fully managed services to quickly add machine learning capabilities to your workloads using API calls. This gives you the ability to build powerful, intelligent applications with capabilities such as computer vision, speech, natural language, chatbots, predictions, and recommendations. Services at this level are based on pre-trained or automatically trained machine learning and deep learning models, so you don't need machine learning knowledge to use them. Know these different services at a high-level and how you can use them. You can use Amazon Translate to translate or localize text content, Amazon Polly for text-to-speech conversion, Amazon Lex for building conversational chat bots, and Amazon Rekognition to add image and video analysis to your applications. Dive deeper into all of the AI services. 

Up next is the machine learning services level that provides managed services and resources for machine learning to developers, data scientists, and researchers. Amazon SageMaker is a focus here and it enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Dive deeper into SageMaker. And a new machine learning service to AWS is Amazon CodeWhisperer, which is a machine learning-powered code generator that provides you with code recommendations in real time and can also scan your code to highlight and define security issues. It probably will not appear on the exam for a few months, but I wanted to include it. 

Up last is the machine learning framework and Infrastructure. In AWS, you can use open-source machine learning frameworks such as TensorFlow, PyTorch, and Apache MXNet. The Deep Learning AMI and Deep Learning Containers in this level have multiple machine learning frameworks pre-installed that are optimized for performance and are ready to be launched on powerful, machine learning-optimized compute infrastructure, such as Amazon EC2 P3 and P3dn instances, that provides a boost of speed and efficiency to machine learning workloads. Amazon machine learning can create machine learning models based on data stored in S3, Amazon Redshift, or RDS. 

For example, you can use AI services for sentiment analysis of customer reviews on your retail website, and use managed machine learning services to build a custom model using your own data to predict future sales. Here is an example of a question you may see on your exam. Which AWS service adds visual analysis features to your application to search, verify, and organize potentially millions of images? Well, the first thing you need to know is that we are looking for an AI Services level service. Your first thought might be Amazon SageMaker, but remember that is a machine learning services level service. Amazon Rekognition is the correct answer because it can add images and video analysis to your applications. You provide the image or video to the Rekognition API and then the service can identify objects, people, text, and so on. 

Let's move on and talk about analytic services. Well, again fundamentals, what is data analytics? Data analytics converts raw data into actionable insights and helps companies gain more visibility and a deeper understanding of their processes and services. It gives them detailed insights into the customer experience and customer problems, and connects insights with actions to create personalized customer experiences, build related digital products, optimize operations, and increase employee productivity. 

Amazon Athena and Amazon Macie are both AWS analytic services. Athena is an interactive query service that allows you to analyze and query data stored in S3 using standard SQL. Athena is considered to be serverless because you do not have to provision anything, you pay per query or per terabyte scanned. There is no need to set up complex extract, transform, load processes and it works directly with your data stored in S3. Athena is great to query log files, great to generate reports, can analyze your cost and usage reports that are stored in S3, and you can run queries on clickstream data. 

Now let's check out Macie, and for the exam if you get a question asking for which AWS service helps with Personally Identifiable Information or PII, choose Macie, and I remember it because Macie has an I and so does PII. PII is personal data used to establish an individual's identity, including your name, home address, email address, social security number, drivers license number, passport number, date of birth, bank account information, credit card information, and more. And this is data that needs to be kept secure so it is not used or exploited. Macie is an AWS security service that uses machine learning to discover, classify, and protect sensitive data stored in S3. 

Macie also uses artificial intelligence to recognize if your S3 objects contain any sensitive PII data and provides dashboards, reports, and alerts and works directly with your data stored in S3. Macie can also analyze your AWS CloudTrail logs. So, for the exam remember, Athena is a query service and Macie is a security service that uses machine learning and artificial intelligence to protect PII. 

I also want to talk about Athena and Redshift. Athena can query huge datasets in S3, with no preparation and only pay for the data that is queried. Now if you are querying in regular databases, you have to create the table structure and the data structure in advance and this is your schema. So, once you have your schema you have to put data into that schema, and your schema cannot be easily changed after you create it, but with Athena this is not the case. In Athena your data stays in S3 in whatever format it is put in and Athena also supports other data formats, like Parquet, XML, JSON, CSV, and more. 

Now for Amazon Redshift. Remember Redshift is a petabyte scale data warehousing solution form AWS, and it is a column-based database engine for analytical workloads that is designed for Online Analytical Processing. Redshift uses a cluster architecture, and unloads and uploads data to S3. It can also accept data from DynamoDB, DMS, and other databases, and Amazon Kinesis too. And to go along with S3, we can use Redshift Spectrum to perform queries directly against S3. This is not like Athena, with Redshift Spectrum you must have a Redshift cluster, but you do not need to load the data into Redshift. You can simply query directly from S3. Athena is serverless so there is not an instance to provision. 

We just mentioned Amazon Kinesis. Let's cover that next. Kinesis processes and analyzes streaming data at any scale as a fully managed service. With Kinesis, you can ingest real-time data, such as video, audio, application logs, website clickstreams, and IoT data, for machine learning, analytics, and other applications. Real time is a keyword for Kinesis. An example of this could be a mobile application that tracks and streams what users tap on and which areas of that application are most used. All of this is streamed and when you only had a few hundred users this stream of data is easy to handle, but what happens when that jumps to millions of users? Well, this is when you could use Kinesis to ingest huge quantities of data in real time and remember it is a fully managed scalable service. Dive deeper into the Kinesis family for more services like Kinesis Data Firehose and Kinesis Data Analytics. 

Let's also talk about AWS Glue because preparing your data to obtain quality results is the first step in an analytics or machine learning project. AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development. It also includes additional productivity and data operations tooling for authoring, running jobs, and implementing business workflows. You can visually create, run, and monitor extract, transform, and load pipelines to load data into your data lakes. Also, you can immediately search and query cataloged data using Amazon Athena, or Amazon Elastic Map Reduce, and also Amazon Redshift Spectrum. 

Another AWS service of focus is Amazon QuickSight which is a fast, business intelligence service that delivers insights to everyone in your organization. As a fully managed service, Amazon QuickSight lets you create and publish interactive dashboards that include machine learning insights. 

Here is a question. Which AWS service supports business intelligence tools such as Apache Spark to perform data transformation workloads and analytics? Well, we just mentioned QuickSight with business intelligence. Is that the correct answer? No. What about Amazon OpenSearch? Also no. Amazon OpenSearch is incorrect because it is a type of database and search engine, and it does not support Apache business intelligence tools and you cannot perform extract, transform, load jobs using these tools alone. But EMR is a web service that enables businesses, researchers, data analysts, and developers to process vast amounts of data. It utilizes a hosted Apache Hadoop framework running on the infrastructure of Amazon EC2 and Amazon S3. EMR can securely and reliably handle broad sets of big data use cases, including machine learning, data transformations, financial and scientific simulation, bioinformatics, log analysis, and deep learning too.



Let's get started with the last task statement and talk about other AWS services that are in-scope for this exam.


Idenitfy services from other in-scope AWS service categories
–
Let's get started with the last task statement and talk about other AWS services that you may see on your exam. Before we jump in to talk about different services, I want to add in AWS monitoring and observability services. 

Part of the fundamentals needed for this course is knowing and understanding the AWS Well-Architected Framework. One of the best practices is staying current and continual improvement of your designs. It is critical for continuous improvement to track and respond to key metrics for applications and the infrastructure in your environment. Know how to use Amazon CloudWatch and AWS X-Ray to observe what is happening in your systems. Know that you can trigger automated actions based on key metrics using CloudWatch alarms. 

Additionally, know how to respond in near real-time to changes in your environment using Amazon EventBridge. Remember, we need to understand the fundamentals of the services under this task statement. 

Ensure you understand each category. For example, application integration on AWS is a suite of services that enable communication between decoupled components within microservices, distributed systems, and serverless applications. So what are some AWS application integration services? Ensure you have depth in Amazon EventBridge, Amazon Simple Notification Service, or Amazon SNS, and Amazon Simple Queue Service, Amazon SQS, and load balancing to decouple workloads. 

In order to scale, you need visibility, so ensure you understand the basics of CloudWatch metrics, alarms, and dashboards. Additionally, look at how metrics can be used as the basis for an Amazon EC2 Auto Scaling. 

For alarms and EventBridge, know how to add in automations and alerts and remediations for scaling events based on alarms created in Amazon CloudWatch. These alarms are triggered when a metric such as an EC2 CPU utilization crosses a certain threshold for a defined amount of time. 

Knowing how to choose what metrics to monitor for scaling is something you should be very familiar with for the real world and if you're interested in taking the AWS Certified Solutions Architect Associate exam. 

Let's dive a bit deeper into a few of these services. Amazon SQS is a queuing system that provides fully managed highly available message queues. And SQS can be used for inter-process, servers, and services messaging. The way a queue works is if you have a queue you add a message to the queue, and then something else retrieves messages from that queue. So it offers an asynchronous way to talk between two components of an application. 

And SQS has two types of polling, short polling and long polling. Short polling is a single API call to check the queue for any messages, and you will get the messages available in the queue up to a maximum of 10 messages or you'll get no messages. 

With short polling, it responds immediately to any messages in the queue, meaning you will need to constantly be sending API calls to check the queue for messages, and this consumes a lot of API calls. 

But you can also use long polling, and this is the same process to check for messages in the queue as short polling; however, you wait for a certain amount of time, and that time is known as the WaitTimeSeconds. And long polling is more efficient because you have a lot less API calls to the queue, but also a lot less of empty API calls. 

SQS queues come in two different types too; standard and first in or first out, also known as FIFO queues. Standard was the AWS original, and FIFO queues were added later. But there are a few differences to know for your exam. Standard queues are distributed and scalable to nearly an unlimited message volume, but the order of the message is not guaranteed, it is best-effort, and messages are guaranteed to be delivered at least once, but sometimes the messages are delivered more than once. You can have duplicate messages or an out of order messages but you will have great performance. 

FIFO queues were introduced to solve this problem. FIFO queues guarantee that messages are delivered in the order that they were received and they are only ever delivered once, so duplicates do not occur. 

Let's also talk about the SQS architecture. SQS is used when you need to decouple two different parts of a system, and decouple is an exam keyword. So if you see decouple, asynchronous messaging, or the ability to have independent scaling of your application on your exam, the correct answer might be SQS. 

Now, let's also dive deeper into Amazon SNS. SNS is a highly available, durable and secure pub sub messaging. It is a public AWS service, so you need network connectivity, and it is accessible wherever network connectivity is available. SNS coordinates the sending and delivery of messages, and SNS is used by many AWS services, such as CloudWatch when alarms change states, auto scaling groups can be configured to send notifications to a topic when a scaling event occurs, and also with CloudFormation when stacks change state and so on. 

Let's also talk about business application services such as Amazon Connect and Amazon Simple Email Service, or Amazon SES. What is a business application service? Well, it is a service that improves agility and lowers cost. Amazon Connect is a contact center that helps to provide customer service for voice and chat across customers and agents. Amazon SES is a cost-effective, flexible, and scalable email service to send mail from within any application. And with Amazon SES you can send emails securely, globally, and at scale. 

Here's a question I saw debated on an internal AWS Slack channel. It was actually for the AWS Certified Database Specialty practice exam but the gist of it was, it was trying to choose between Amazon SNS and Amazon SES. The question was asking for a notification with immediate and the least effort to configure. So remember to ensure you understand exactly what the question is asking when you're reviewing the questions and answer responses. Amazon SES is a great choice if you need the email to come from a custom domain. 

What are some customer engagement services and what are they used for? Customer engagement services help to create the best customer experience across the entire life cycle. AWS provides AWS Activate for Startups, AWS IQ, AWS Managed Services, and AWS Support. AWS Activate provides eligible startups with free tools, resources, and content, designed to simplify every step of the startup journey. AWS IQ connects you to an AWS certified expert for hands-on help for your AWS projects. AWS Managed Services is infrastructure operations management for AWS, and it is an enterprise service that provides ongoing management of your AWS infrastructure. AWS Support offers a range of plans that provide access to tools and expertise that support the success and operational health of your AWS solutions. AWS support plans provide 24/7 access to customer service, AWS documentation, technical papers, and support forums. For technical support and more resources to plan, deploy, and improve your AWS environment, you can choose a support plan for your AWS use cases. 

What about developer services? What is a developer service, and what services and tools does AWS offer? Developer services and tools are designed to help developers practice DevOps and deliver and to deploy. Some of the services that AWS provides are: AWS AppConfig, AWS Cloud9, AWS CloudShell, AWS CodeArtifact, AWS CodeBuild, AWS CodeCommit, AWS CodeDeploy, AWS CodePipeline, AWS CodeStar, and AWS X-Ray. How familiar are you with these services? Ensure you dive deeper. 

What about end-user computing services? What are they and what are they used for? End-user services provide secure access to the application and desktops needed. Some services in AWS are Amazon AppStream 2.0, Amazon WorkSpaces, and Amazon WorkSpaces Web. Amazon AppStream 2.0 is a fully managed application streaming service that provides users with instant access to their desktop applications from anywhere. Amazon WorkSpaces helps to provision virtual cloud-based Microsoft Windows, Amazon Linux, or Ubuntu Linux desktops for your users known as WorkSpaces. Amazon WorkSpaces Web is an on-demand, fully managed, Linux-based service designed to facilitate secure browser access to internal websites and software-as-a-service applications. 

We're almost done, but let's also talk about AWS services for front-end web and mobile services to help developers develop workflows and deliver and deploy securely. AWS Amplify is a complete solution that lets front-end web and mobile developers easily build, ship, and host, full-stack applications on AWS. AWS AppSync provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs. 

Let's wrap up this lesson and talk about IoT services such as AWS IoT Core and AWS IoT Greengrass. What are IOT services used for? Well, they help to securely connect and manage devices, collect and analyze device data, and build and deploy solutions. AWS IoT provides device software that can help you integrate your IoT devices into AWS IoT-based solutions. AWS IoT Greengrass is a software that extends cloud capabilities to local devices to collect and analyze data closer to the source of the information, react autonomously to local events, and communicate securely with each other on local networks. Local devices can also communicate securely with AWS IoT Core and export IoT data to AWS. For this task statement, ensure you can choose the appropriate service for requirements and use cases as well as choosing the appropriate support options for your business. 

Let's get started with the fifth walkthrough question.


Walkthrough question 5
–
Let's get started with our fifth walkthrough question which is from task statement 3.1: define methods of deploying and operating in the AWS Cloud. 

The question reads: A company wants a dedicated private connection to the AWS Cloud from its on-premises operation. Which AWS service or feature will provide this connection? 

Reading this question, can you identify any keywords or phrases? And exactly what the question is asking? A few keywords I see are dedicated private connection. Now we mentioned an AWS service and a dedicated private connection being a keyword for that service. Do you remember which AWS service that is? 

Well, let's see if you were paying attention and explore the responses. 

Option A, AWS VPN
Option B, AWS PrivateLink
Option C, VPC endpoint
And option D, AWS Direct Connect. 
Pause the video if you need more time. Okay, let's evaluate the options. 

Option A is incorrect. AWS VPN establishes a secure connections between your on-premises network, remote offices, client devices, and the AWS global network. AWS VPN is not a dedicated connection. 



Option B is incorrect. You use PrivateLink when you want to use services offered by another VPC securely within the AWS network. With PrivateLink, all network traffic stays on the global AWS backbone and never traverses the public internet. PrivateLink does not connect to on-premises operations. 



Option C is incorrect. A VPC endpoint enables private connections between your VPC and supported AWS services and VPC endpoint services powered by PrivateLink. A VPC endpoint does not connect to on-premises operations. 



So that makes option D correct. Direct Connect provides a dedicated private connection from your on premises to the AWS Cloud. Direct Connect is an alternative to using the internet to access AWS services. 



That's all for this question. 



Be sure to take note of any knowledge gaps that you may have identified while exploring this question. 



Let's get started with our sixth walkthrough question.


Walkthrough question 6
–
Let's get started with our sixth walkthrough question which is from task statement 3.2 define the AWS global infrastructure. 

The question reads, which aspect of AWS infrastructure provides global deployment of compute and storage? Reading this question, can you identify any keywords or phrases and also exactly what the question is asking?

 A few keywords I see are global deployment of compute and storage. 

Now that we've examined the question, identified the keywords and reviewed the requirements, let's explore the responses. 

Option A is mulitple Availability Zones in an AWS Region.
Option B is multiple AWS Regions.
Option C is tags.
Option D is resource groups.
Option A is incorrect. Availability Zones are one or more discrete data centers with redundant power networking and connectivity in a Region. When infrastructure is deployed across multiple Availability Zones, you can achieve a highly available deployment within a geographical location of the Region. However, this solution does not provide global deployments. 



Option B is multiple AWS Regions. A Region is a physical location where there are clusters of AWS data centers. AWS offers many different Regions where you can deploy infrastructure around the world. With the use of multiple Regions, you can achieve a global deployment of compute, storage, and databases. This makes option B a good candidate for the correct key but let's look at the other responses. 

Option C is incorrect. Tags are metadata that you can associate with your AWS resources. Tags are user-defined data in the form of key value pairs. You can use tags to manage, identify, organize, and search for, and filter resources. Tags do not provide global deployments of applications and solutions. 

Option D is also incorrect. AWS resource groups is a service that you can use to manage and automate tasks on many resources at the same time, resources in AWS are entities such as an Amazon EC2 instance and Amazon S3 buckets. With resource groups, you can filter resources based on tags or AWS CloudFormation stacks and then perform an action against a group of resources. You do not use resource groups to deploy AWS resources globally. 

That makes option B the correct answer.

 That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question, and let's get started with Domain 4. Billing, pricing, and support.

Domain 4: Billing, Pricing, and Support

This module has six videos.


Domain 4 Introduction
–
Let's get started with domain 4, covering billing, pricing, and support. This domain covers how AWS billing and pricing plays out in the real world. 

Remember, fundamentally, cost and billing shifts in AWS from running on premises. When you run on-premises environments for technical resources like an on-premises database, that generally means you have a fixed-cost model because you purchase the hardware upfront and then pay it down over time and pay the operating expenses month to month, which are predictable and planned for. When you shift to using AWS to host technical resources, that pricing model changes. Instead of costs being generally fixed month to month, you will now be working with a variable pricing model, meaning that costs can change from month to month. 

Domain 4, billing, pricing, and support, is broken into three task statements that we will discuss over the next few lessons. 

Task statement 4.1, compare AWS pricing models
Task statement 4.2, understand resources for billing, budget, and cost management
Task statement 4.3, identify AWS technical resources for AWS support options. 
For the first task statement, we will focus on AWS pricing models. As you probably know by now, in AWS, you only pay for what you use, and what you use may change from month to month depending on your usage patterns. This variable pricing model means a few things. First, it may be more challenging to predict why you get used to the different pricing structures within AWS, but it also means that you will have the power to influence your AWS bill month to month by changing your architectures, usage patterns, and approaches to solving problems. 

For the second task statement, ensure you know you can estimate, track, break down, and influence your AWS bill, and understand resources for billing, budget, and cost management. 

For the third task statement, ensure you can identify resources available for billing support and AWS support options. 

Over the next several videos in this module, I will address each task statement individually, breaking down the knowledge and skills expected of you to be successful. 

Let's start to evaluate your readiness for the exam in the next video, where we will cover the first task statement from domain 4 to compare AWS pricing models.


Compare AWS pricing models
–
Let's get started with the first task statement for domain 4, comparing AWS pricing models. In an earlier lesson, we talked about the AWS Well-Architected Framework and the six pillars, and we mentioned that we would dive deeper into the cost optimization pillar in domain 4. 

So again, fundamentals. What is cost optimization? Cost optimization is the ability to run systems that deliver business value at the lowest price point. AWS offers multiple ways to run a cost-optimized environment with AWS services such as AWS Budgets, AWS Cost and Usage Reports, AWS Cost Explorer, Reserved Instances, Reserved Instance Reporting, and more. AWS also provides cost effective resources like Spot Instances, Reserved Instances, and cost-effective storage like Amazon S3, and S3 Glacier and Glacier Deep Archive. 

AWS also allows us to match supply with demand with AWS Lambda, Amazon EC2 Auto Scaling, and AWS Auto Scaling. And you can optimize over time using AWS Trusted Advisor, Cost and Usage Reports and more. 

The cost optimization pillar has design principles and best practices that will help with this domain but especially in the real world. As always, fundamentals matter. 

So, let's quickly talk about the cost optimization pillar. In order to achieve cost optimization, you must ensure you are rightsizing your infrastructure. 

Rightsizing is picking the correct instances for your current resources, but also for resources you plan to use. So maybe you are using a larger EC2 instance size when all you need to cover your demand is a small instance size. Rightsizing and choosing the correct instance type but also the cheapest instance type that meets performance requirements can save you money. You must also ensure you are increasing elasticity and only using resources when those resources are needed, which gives you a pay-for-what-you-use model. 

Again, using smaller instances versus fewer larger instances for your workload can reduce your costs, but also using auto scaling to scale up your instances when the demand scales and then scale back down when the demand lessens reduces cost even further. 

Another cost optimization necessity is choosing the right pricing model, which is the focus for this task statement, and choosing the right pricing model comes into play after you have right-sized your instances and set up auto scaling. AWS offers several different pricing models such as Reserved Instances, On-Demand Instances, Spot Instances, Saving Plans, Dedicated Hosts, Dedicated Instances, and Capacity Reservations. 

On-Demand Instances are just as they sound, you can choose to use these when they're needed. They're great if you have a flexible start and end time and are great for applications with uncertain requirements, and also great for short-term workloads. It is also great for applications that cannot tolerate a disruption and that is a key phrase for the exam. But you can save money by choosing Reserved Instances, but Reserved Instances come with a 1- or 3-year commitment. However, with that commitment you receive a lower price. 

And then there are Spot Instances, which are the cheapest pricing model with up to 90% off the On-Demand pricing. AWS uses their spare EC2 capacity and sales at capacity in Spot Instances, and the price is based on how much extra capacity there is. So your spot price changes as the capacity changes. Spot Instances are great with flexible applications, applications that can withstand a disruption or tolerate a failure. And for applications that need a really low price. Spot Instances are the cheapest pricing model but applications have to be flexible with interruptions. 

Reserved Instance pricing can save you up to 72% on the On-Demand price but you have that 1- to 3-year commitment. The 3-year term is more cost effective than the 1-year, and you can pay your Reserved Instances all upfront, no upfront, or partial upfront. And it is important to know that you pay this reservation whether you have instances running or not. 

Reserved Instances can be used across your Regions or across your Availability Zones using Capacity Reservations. Regional Reserved Instances apply the discount to any Availability Zone in the Region, along with Convertible Reserved Instances that are a type of Reserved Instances that add additional flexibility to change instance families, operating systems, or tendencies over the Reserved Instance term. 

You can also schedule your reservations where your rate is reduced during a time window, so you pay for the commitment within that schedule. Schedule Reserved Instances help to match your capacity reservation to a predictable reoccurring schedule. 

For the exam, know that Standard Reserved Instances provide the most significant discount compared to On-Demand Instances.



For billing purposes, the consolidated billing feature of the AWS Organization treats all the accounts in the organization as one. This means that all accounts in the organization can receive the hourly cost benefit of Reserved Instances that are purchased by any other account. 

Reserved Instances are great for applications with steady state usage, applications with long-term needs and are much cheaper than On-Demand Instances, and you can actually use all of these pricing models at the same time. So, Reserved Instances can cover your production environment. 

Spot Instances can be used when you have an increase in your demand, and you can use On-Demand Instances as needed. This is a great way to optimize your environment by knowing and understanding the pricing model, and choosing the right pricing model for your environment. 

Let's look at some other pricing models too. Dedicated Hosts, which we've mentioned already a few times, is considered a pricing model but it is more of a technical difference. Remember, Dedicated Hosts are reserved for you alone and you pay for that Dedicated Host to be dedicated to you, and it is not shared with other AWS customers. And with Dedicated Hosts, you pay for the host, not the individual instances running on that host. Dedicated Instances are instances that you pay per hour for the instance running on a single tenant. 

Scheduled Instances are great for applications that need to be available on a regular schedule. Saving Plans are great for your compute usage and you can use the Saving Plans for Lambda and Amazon ECS2. 

AWS also offers a service called AWS Trusted Advisor, which monitors your infrastructure and can make recommendations on how to make your infrastructure more optimized. And then, of course, you can use AWS Cost Explorer to monitor your cost. 

Let's jump back to the cost optimization pillar. Along with right-sizing you must match your storage to your usage. Reducing your storage can save money and you can match your storage usage to a particular storage class. Again, AWS offers multiple storage classes so ensure you pick the correct one such as Amazon S3, Amazon EBS, AWS Storage Gateway, Amazon EFS, CloudFront and multiple data transfer options too. And then, of course, we must calculate our data transfer in and out of the AWS Regions. 

AWS offers tiered pricing for data transfer from AWS to the internet to help reduce data transfer costs. Data traffic that is destined to your on-premises data center is considered data transfer out and it results in a charge to your monthly bill. You can architect parts of your infrastructure to use AWS services like CloudFront and Direct Connect. For example, you can eliminate these data transfer out charges by using Direct Connect because cost associated with data traffic through the private connection are included in the service. Likewise, if you use Amazon S3 for content delivery, using CloudFront will reduce the latency between your end users and your S3 bucket. And the key point is that by continuously assessing your AWS architecture and understanding where your data transfer costs are incurred, you can adapt your deployment to reduce overall running costs while improving the end user experience. 

Lastly, ensure you measure and monitor your infrastructure because it will most likely be changing, so you need measures in place to monitor and measure your usage and your costs. You can monitor your utilization of CPU, RAM, storage and more, to identify instances that could be downsized or may need to be increased. AWS provides CloudWatch to track your metrics and to set alarms to immediately take actions. You also have Trusted Advisor that we mentioned just a minute ago. 

The AWS Well-Architected Framework Tool and Cost Explorer. It is crucial to define your metrics, set target goals, define and enforce your tagging strategy, use cost allocation tags and make sure you are reviewing regularly for any infrastructure changes. 

For this task statement, dive deeper into the cost optimization design principle that we have covered a few times already. Let's also cover cost optimization best practices. First, define and enforce cost allocation tagging throughout your whole environment so you can see what resources are costing more. 

AWS also recommends using an effective account structure, so this means you need to know what are your end goals and make sure you're designing to meet those goals and define and use metrics to track the progress of meeting those goals. Another huge best practice is to enable your teams to design their architecture based on cost. So, let your teams know what the cost is and give them access so they can see what they're building and what costs are associated with that build. Once you give a team, or person, ownership of cost you will find that most are much more cost conscious. And lastly, AWS recommends you create a CCOE, a Cloud Center of Excellence, and this is a team who stays up to date on AWS best practices, new releases and more, to ensure you're using AWS in the most efficient and cost-effective ways. 

When you open up your expenditure you make your teams more accountable and more aware to choose more cost-effective resources that still match your supply to your demand, but also make sure you are optimizing your AWS account and infrastructure over time. 

Let's get started with the second task statement and talk about resources for billing, budget, and cost management.


Understand resources for billing, budget, and cost management
–
Let's get started with the second task statement to understand resources for billing, budget, and cost management. In our last lesson, we talked about the need to implement a good monitoring and cost management solution to understand your resource needs, how costs are allocated, and to measure and monitor your architecture. 

We also mentioned that your architecture will most likely be changing so you need measures in place to monitor and measure your usage and your cost. Using AWS Cost Explorer and the AWS Cost and Usage Reports, you can monitor your utilization of CPU, RAM, storage, and so on to identify instances that could be downsized or may need to be increased. 

We have mentioned these services several times already but AWS provides CloudWatch to track our metrics and alarms to immediately take actions. There are other services too but it is crucial to define your metrics, set target goals, define and enforce your tagging strategy, use cost allocation tags, and make sure you are regularly reviewing for any infrastructure changes. 

This will require a good understanding of cost management tools. For example, how do you use cost allocation tags? One way is to use tags to filter views in Cost Explorer to help analyze costs. For example, if you wanted high level interactive financial reports would you use AWS Cost Explorer or the AWS Cost and Usage Report? Cost Explorer gives you a high-level view that you can then drill down into more specifics. AWS Cost and Usage Reports breakdown costs by the hour, day, month, product, resource tags, and so on to provide the most granular data about your AWS cost and usage, and you can send that data to Amazon Athena, Amazon Redshift, AWS QuickSight, or another tool to perform quick queries and more. 

You can also create billing alarms, free tier alarms, and alarms with AWS Budgets. Know different ways you can trigger automated actions based on defined budget thresholds. This can help you keep a close eye on the state of your AWS bill and be alerted if you're exceeding any cost threshold set for the account. 

AWS Organizations and Control Tower help to centrally manage billing, control access, compliance, security, and share resources too. Ensure you know the benefits of consolidated billing. You can also use data visualization tools like Amazon QuickSight to analyze AWS Cost and Usage Reports, or to create custom reports. So, you're going to want to ensure you know the high-level functionality of Amazon QuickSight. 

Be sure to know how to use AWS Auto Scaling and Amazon EC2 Auto Scaling to help minimize costs by ensuring you only have the appropriate number of instances to match your customer demand. 

Know other AWS services, such as AWS Lambda and Amazon API Gateway, that can automatically scale up and down. Again, Trusted Advisor can give recommendations when it finds underutilized EBS volumes in your account. Having monitoring in place to establish a baseline of your usage is a big part of being able to rightsize your volumes. Ensure you continue to track the correct metrics to determine if you need to modify your solution for cost optimization. 

You can also optimize the cost for EBS by paying attention to how many IOPS you're using and then comparing that to how many IOPS you're paying for. If you are paying for the most optimized, most performant volume type but are not using that capacity, then you may be able to change your volume type to save money without taking a performance hit. 

Know how you can use data lifecycle rules to automatically delete data that is no longer needed to reduce your storage costs and be prepared to choose the most cost-effective storage when provided a set of requirements. 

Check out the services Amazon Data Lifecycle Manager and AWS Backup to learn more about solutions for automatically deleting old EBS snapshots or backups from other AWS services as this could help you reduce the storage costs within an AWS account. 

For Amazon S3, you can optimize for costs by making use of the different storage classes when appropriate. You should be familiar with the different S3 storage classes and how they impact both cost and retrieval times. You don't need to know the exact pricing for the S3 storage tiers, but you should know the trade-offs that are made between storage costs and retrieval costs for each tier, and know how that can impact your AWS bill, given a scenario for data storage and access patterns. 

Here's a question. Amazon S3 has a wide variety of storage classes that you can use to minimize cost based on your use case. Which Amazon S3 capability allows you to automatically transition all objects in a bucket based after a period of time? S3 Lifecycle Configuration or Intelligent Tiering? Lifecycle policies can help with this. Which S3 storage class will automatically move each individual object to the most cost-effective access tier when access patterns change? Intelligent Tiering is a great storage class for this requirement, but remember you can create lifecycle policies with other S3 storage classes too. 

Dive deeper into S3, you will most likely get a few storage cost optimization questions where S3 is the answer or a plausible distractor. Is it possible to configure an S3 bucket to have the requestor pay the cost of the request and the data downloaded in instead of the S3 bucket owner? Yes, it is if you configure the requester to pay for storage transfers and usage. 

You also need to know the different options for optimizing the data migration calls for hybrid environments like AWS DataSync, the Snow family, the AWS Transfer family, and AWS Storage Gateway. If your organization decides to move 250 terabytes of archive data from their internal servers to S3, what is the fastest and most cost-effective way to import the data to S3? AWS Snowmobile, establish a Direct Connect connection to transfer the data to S3, order multiple Snowball devices, or upload it directly using their current one hundred megabits per second dedicated line? The Snowball is usually a strong choice for data transfer, especially if you need that transfer to be more secure and quickly transfer terabytes to petabytes of data to AWS. Architecting for data transfer ensures that you minimize data transfer costs. 

Let's say you are building a data lake using Amazon S3 as your storage platform. What data transfer would incur costs? When you transfer data from Amazon S3 to the internet, you are charged for the storage size per month and the Region that the object will be transferred to. Here is another question. Will AWS charge you for data transferred between two Regions? Yes, AWS charges you for data transferred between two different Regions. This is similar to the costs incurred from the data transfer between the AWS network and the public internet. 

Also, know that you can estimate costs to migrate and run your workloads on AWS with the AWS Pricing Calculator that provides an esteemed monthly cost of AWS services for your use case based on your expected usage. Here is another question. Which service offers volume discounts when you enable consolidated billing? Remember that consolidated billing enables you to share resources, see a combined view of AWS costs incurred by all accounts in your department or company, as well as obtain a detailed cost report for each individual AWS account associated with your paying account. 

For billing purposes, AWS treats all the accounts in the AWS Organization as if they were one account. Some services, such as Amazon EC2 and Amazon S3, have volume pricing tiers across certain usage dimensions that give you lower prices the more you use the service. With consolidated billing, AWS combines the usage from all accounts to determine which volume pricing tier to apply, giving you a lower overall price whenever possible and then allocates each linked account a portion of the overall volume discount based on the account's usage. 

Let's wrap up this lesson and talk about another service. The AWS Billing Conductor, which is a custom billing service that can support the showback and chargeback workflows of AWS Solution Providers and Enterprise customers. 

Let's get started with our last test statement for this course which is to identify AWS technical resources and AWS support options.


Identify AWS technical resources and AWS Support options
–
Let's get started with our last task statement, which is to identify AWS technical resources and AWS support options. When it comes to using AWS, it's impossible for any one person to know absolutely everything there is to know about all of the services, features, and tools, even with the Cloud Center of Excellence. And when you run into things you can't figure out, or solve for yourself, or if you just need something that would be too difficult or time consuming to build yourself, you have the AWS Managed Services and the technology support options. 

Support in this context doesn't just mean that something has gone wrong and you need help fixing it. Oftentimes, support involves assisting as you design and build your environments, so that you are able to get the assistance and guidance that you need. For example, AWS Enterprise support provides you with a concierge service where the main focus is to help you be successful in AWS. You get 24/7 technical support from engineers, tools, technology to automatically manage the health of your environment, a designated technical account manager to coordinate access to proactive and preventative programs, and AWS subject matter experts. For companies with a Business support plan, you can have access to infrastructure event management for an additional fee. 

However, all other proactive support programs, such as the well-architected reviews and operation reviews are exclusively available for companies who opted for Enterprise support. Here's a question to test your depth on AWS support plans. 

What if you need the most cost-effective AWS support plan with access to the AWS support API for programmatic case management, which support plan do you choose? AWS Developer, Enterprise, Enterprise On-Ramp, Basic, or Business? Will all AWS customers automatically have around-the-clock access to these features of the Basic support plan, Customer Service, 1-on-1 responses to account and billing questions, support forums, service health checks, documentation, whitepapers and best practice guides? In addition, customers with a Business or Enterprise support plan have access to these features. Use case guidance, so what AWS products, features, and services to use to best support your specific needs, AWS Trusted Advisor, which inspects customer environments, then Trusted Advisor identifies opportunities to save money, close security gaps, and improve system reliability and performance. An API for interacting with support center and Trusted Advisor. This API allows for automated support case management and Trusted Advisor operations, third-party software support, so help with Amazon EC2 instance operating systems and configuration, also help with the performance of the most popular third-party software components on AWS. 

So the Business plan would be the correct answer, because it is more cost effective than Enterprise support. 

Let's look at one more question on support plans. Which support plan allows an unlimited number of technical support cases to be opened: Basic, Business, or Developer? The most cost-effective support plan that offers a technical support with an unlimited number of cases that can be opened is the Developer support plan. Additionally, it provides you access to the seven core Trusted Advisor checks, and the Personal Health Dashboard where you get a personalized view of the health of AWS services and alerts you when your resources are impacted. 

There is also other support, such as all of the AWS documentation that exists along with AWS re:Post, whitepapers, blogs, and all of the other areas of AWS documentation. It is important to know where you can find the answers you are looking for. And here is a huge real-world tip and probably the best advice I received from my mentor. Learn how to Google and search for the answers or guidance you need. 

In other words, how is your Google kung fu? Mine's pretty strong. 

Where do you go if you need guided instructions on how to deploy a service for the first time? My first choice is AWS documentation. 

What do you look for for AWS best practices? My first choice is the AWS Well-Architected Framework, and then the AWS documentation for the service that I'm interested in. 

How do you seek community support when you have a very specific question about what you're building? My first choice would be AWS re:Post. 

Additionally, you can utilize AWS Training and Certification to help you build your knowledge and experience with AWS Skill Builder and all of our digital training and hands-on labs. 

Understanding where to find your documented answers is the first step in seeking support. After that, there are paths you can use to find assistance with more urgent matters. Evaluate how you could use the various teams within AWS Abuse, premium support, and billing or account support. 

When can you utilize the technical account managers, and how can they help you with your environment? Additionally, understand what the AWS Partner Network is and how you can use that too. 

For all of the support areas I've mentioned, it's important to be able to identify the sources of technical assistance, and knowledge you have at your service. The documented knowledge, community, professional services, the AWS Partner Network, and more are there to help our customers succeed. 

This area will be where you likely get the most practice and your Google kung fu will grow as you perform your searches. Knowing where to find supporting knowledge will probably be one of the most important things you can learn as you prepare for the exam and will greatly aid you as you use AWS. 

Let's get started with our seventh walkthrough question.


Walkthrough question 7
–
Let's get started with our seventh walkthrough question, which is from task statement 4.1: Compare AWS pricing models. 

The question reads, "A company must meet compliance and software licensing requirements that state a workload must be hosted on a physical server. Which Amazon EC2 instance pricing option will meet these requirements?" 

Reading this question, can you identify any keywords or phrases and exactly what the question is asking? A few keywords I see are compliance and software requirements and physical server. 

Now that we've examined the question, identified key words, and reviewed the requirements, let's explore the responses. 

Option A, Dedicated Hosts
Option B, Dedicated Instances
Option C, Spot Instances
And Option D, Reserved Instances. 
Pause the video if you need more time. Okay, let's evaluate the options. 

Option A is Dedicated Hosts. An EC2 Dedicated Host is a physical server with EC2 instance capacity that is fully dedicated to your use. This makes Option A a good candidate for the correct answer, but let's look at the rest of the responses. 

Option B is incorrect. Dedicated Instances are EC2 instances that run in a VPC on hardware that is dedicated to a single customer. Other instances for that customer can be hosted on the same hardware. 

Option C is incorrect. With Spot Instances, you can take advantage of unused EC2 capacity in AWS. Spot Instances are available at up to a 90% discount compared to on-demand instance pricing. 

And Option D is also incorrect. Reserved Instances provide you with significant savings on your EC2 costs compared to on-demand instance pricing. 

However, Reserved Instances are not hosted on a physical server, so that makes Option A correct. 

That's all for this question. 

Be sure to take note of any knowledge gaps that you may have identified while exploring this question, and let's get started with our eighth walkthrough question.


Walkthrough question 8
–
Let's get started with our eighth walkthrough question which is from task statement 4.2: Understand resources for billing, budget, and cost management. 

The question reads, what is an advantage of consolidated billing on AWS? 

Reading this question, can you identify any keywords or phrases and exactly what the question is asking? A few keywords I see are consolidated billing. Consolidated billing is another keyword for an AWS service. Do you remember which one? 

Now that we've examined the question, identified keywords, and reviewed the requirements, let's explore the responses. 

Option A, volume pricing qualification. 
Option B, shared access permissions. 
Option C, multiple bills for each account. 
And option D, elimination of the need to tag resources. 
Pause the video if you need more time. Okay, let's evaluate the options. 

Option A is volume pricing discounts. Consolidated billing is a feature of AWS Organizations and a keyword. You can combine the usage across all accounts in your organization to share volume pricing discounts, reserved instance discounts, and saving plans. This solution provides a lower cost compared to the use of individual standalone accounts. This makes option A a good candidate for the correct answer, but let's look at all of the responses. 

Option B is incorrect. Shared access permissions is a feature of roles that are developed in AWS IAM. This solution is not related to consolidated billing. 

Option C is incorrect. The goal of consolidated billing is to have one bill for multiple accounts. 

And option D is incorrect. In consolidated billing, you can apply tags that represent business categories. This functionality helps you organize your cost across multiple services within consolidated billing. 

So that makes option A the correct answer. 

That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question, and let's wrap up this course in our next lesson.

Course Close

This module has one video.


Call to action
–
Welcome back, and great job completing this course. I hope you've enjoyed this exam prep course for the AWS Certified Cloud Practitioner Certification. 

Throughout the course, we have provided valuable exam basics and guidance on how to approach the various domains from the exam guide that will be covered in the certification exam. 

As a reminder, this course was not intended to teach what will be on the exam, but instead to provide you a method for self-evaluation to determine your level of readiness to take the certification exam. 

Use the information provided in this course to help guide you in your studies and preparations, and do not forget to get some hands-on experience too. 

AWS provides official practice exams for this certification, and similar to this course, it will help you assess your readiness for the exam and highlight areas where you may still have gaps in your knowledge. 

Let's cover a few test-taking tips. 

First, read and understand the questions before looking at the answer responses. Identify keywords and phrases and qualifiers. This is very important. If the question is looking for the lowest cost option and you are thinking of the most resilient solution, you might choose a different answer. Eliminate some of the answer options based on what you know about the topic. Compare and contrast the remaining options, keeping in mind that the key phrases and qualifiers identified. If you're spending too much time, pick your best guess and flag the question for later review. You get zero points if you leave it blank. 

Remember that the AWS Exam Prep team also has other exam prep courses, and if you're looking for more in-depth and guided instructions and courses, keep in mind that the AWS Training and Certification team creates many great courses from foundational to the professional level and also provides many AWS certifications, again from the foundational, associate, professional, and specialty levels. 

For next steps, I recommend studying any areas that you have identified as gaps. When you're ready to take the test, visit our certification site to schedule your exam. 

Good luck with your studies and preparations and good luck on your exam. Feel free to reach out to me at any time, and remember, AWS is here for you and we are all cheering for you. 

One last quick note, please complete our feedback survey or reach out to me directly with any and all feedback. Feedback is so important to ensure that we are creating content that you need. 



Again, I wish you all the best. Now go crush your exam!!

Thank you

We are eager to hear about your experience with these resources. Return to the course and complete our feedback survey to help us improve our learning content.
